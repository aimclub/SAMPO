{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:42.401002600Z",
     "start_time": "2023-10-12T09:59:42.381523500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sampo.scheduler.selection.neural_net import NeuralNetTrainer, NeuralNet, NeuralNetType\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'datasets/wg_contractor_dataset_100_objs.csv')\n",
    "train_dataset = pd.read_csv(path, index_col='index')\n",
    "for col in train_dataset.columns[:-1]:\n",
    "    train_dataset[col] = train_dataset[col].apply(lambda x: float(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:42.578031800Z",
     "start_time": "2023-10-12T09:59:42.549001200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "train_dataset['label'] = train_dataset['label'].apply(lambda x: [int(i) for i in x.split()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:43.032970600Z",
     "start_time": "2023-10-12T09:59:42.998970300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2          3    4         5         6  \\\nindex                                                                  \n0      115.0  0.901729  0.313043  30.930435  7.0  0.008696  0.008696   \n1      138.0  0.811852  0.355072  29.170290  7.0  0.007246  0.007246   \n2      122.0  1.042841  0.344262  32.971311  7.0  0.008197  0.008197   \n3      115.0  0.952107  0.321739  31.969565  7.0  0.008696  0.008696   \n4       46.0  1.053998  0.586957  33.021739  6.0  0.021739  0.021739   \n...      ...       ...       ...        ...  ...       ...       ...   \n95     137.0  0.987916  0.321168  32.160584  7.0  0.007299  0.007299   \n96     214.0  0.808632  0.373832  29.126168  7.0  0.004673  0.004673   \n97      36.0  1.547846  0.638889  40.277778  6.0  0.027778  0.027778   \n98      36.0  1.853903  0.611111  46.027778  6.0  0.027778  0.027778   \n99      45.0  1.212873  0.600000  35.244444  6.0  0.022222  0.022222   \n\n              7         8         9        10        11        12  \\\nindex                                                               \n0      0.026087  0.330435  0.478261  0.113043  0.026087  0.008696   \n1      0.021739  0.318841  0.442029  0.173913  0.021739  0.007246   \n2      0.024590  0.360656  0.418033  0.147541  0.024590  0.008197   \n3      0.026087  0.339130  0.443478  0.139130  0.026087  0.008696   \n4      0.043478  0.586957  0.282609  0.021739  0.021739  0.000000   \n...         ...       ...       ...       ...       ...       ...   \n95     0.021898  0.306569  0.437956  0.189781  0.021898  0.007299   \n96     0.018692  0.313084  0.481308  0.163551  0.018692  0.004673   \n97     0.055556  0.638889  0.194444  0.027778  0.027778  0.000000   \n98     0.055556  0.611111  0.222222  0.027778  0.027778  0.000000   \n99     0.044444  0.600000  0.266667  0.022222  0.022222  0.000000   \n\n                          label  \nindex                            \n0       [42, 27, 5, 42, 81, 27]  \n1       [47, 31, 5, 47, 92, 31]  \n2      [68, 44, 5, 68, 133, 44]  \n3       [43, 28, 5, 43, 83, 28]  \n4       [40, 26, 5, 40, 78, 26]  \n...                         ...  \n95     [54, 35, 5, 54, 105, 35]  \n96     [58, 38, 5, 58, 113, 38]  \n97     [54, 35, 5, 54, 105, 35]  \n98     [65, 42, 5, 65, 128, 42]  \n99      [44, 28, 5, 44, 86, 28]  \n\n[100 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115.0</td>\n      <td>0.901729</td>\n      <td>0.313043</td>\n      <td>30.930435</td>\n      <td>7.0</td>\n      <td>0.008696</td>\n      <td>0.008696</td>\n      <td>0.026087</td>\n      <td>0.330435</td>\n      <td>0.478261</td>\n      <td>0.113043</td>\n      <td>0.026087</td>\n      <td>0.008696</td>\n      <td>[42, 27, 5, 42, 81, 27]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>138.0</td>\n      <td>0.811852</td>\n      <td>0.355072</td>\n      <td>29.170290</td>\n      <td>7.0</td>\n      <td>0.007246</td>\n      <td>0.007246</td>\n      <td>0.021739</td>\n      <td>0.318841</td>\n      <td>0.442029</td>\n      <td>0.173913</td>\n      <td>0.021739</td>\n      <td>0.007246</td>\n      <td>[47, 31, 5, 47, 92, 31]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>122.0</td>\n      <td>1.042841</td>\n      <td>0.344262</td>\n      <td>32.971311</td>\n      <td>7.0</td>\n      <td>0.008197</td>\n      <td>0.008197</td>\n      <td>0.024590</td>\n      <td>0.360656</td>\n      <td>0.418033</td>\n      <td>0.147541</td>\n      <td>0.024590</td>\n      <td>0.008197</td>\n      <td>[68, 44, 5, 68, 133, 44]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115.0</td>\n      <td>0.952107</td>\n      <td>0.321739</td>\n      <td>31.969565</td>\n      <td>7.0</td>\n      <td>0.008696</td>\n      <td>0.008696</td>\n      <td>0.026087</td>\n      <td>0.339130</td>\n      <td>0.443478</td>\n      <td>0.139130</td>\n      <td>0.026087</td>\n      <td>0.008696</td>\n      <td>[43, 28, 5, 43, 83, 28]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>46.0</td>\n      <td>1.053998</td>\n      <td>0.586957</td>\n      <td>33.021739</td>\n      <td>6.0</td>\n      <td>0.021739</td>\n      <td>0.021739</td>\n      <td>0.043478</td>\n      <td>0.586957</td>\n      <td>0.282609</td>\n      <td>0.021739</td>\n      <td>0.021739</td>\n      <td>0.000000</td>\n      <td>[40, 26, 5, 40, 78, 26]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>137.0</td>\n      <td>0.987916</td>\n      <td>0.321168</td>\n      <td>32.160584</td>\n      <td>7.0</td>\n      <td>0.007299</td>\n      <td>0.007299</td>\n      <td>0.021898</td>\n      <td>0.306569</td>\n      <td>0.437956</td>\n      <td>0.189781</td>\n      <td>0.021898</td>\n      <td>0.007299</td>\n      <td>[54, 35, 5, 54, 105, 35]</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>214.0</td>\n      <td>0.808632</td>\n      <td>0.373832</td>\n      <td>29.126168</td>\n      <td>7.0</td>\n      <td>0.004673</td>\n      <td>0.004673</td>\n      <td>0.018692</td>\n      <td>0.313084</td>\n      <td>0.481308</td>\n      <td>0.163551</td>\n      <td>0.018692</td>\n      <td>0.004673</td>\n      <td>[58, 38, 5, 58, 113, 38]</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>36.0</td>\n      <td>1.547846</td>\n      <td>0.638889</td>\n      <td>40.277778</td>\n      <td>6.0</td>\n      <td>0.027778</td>\n      <td>0.027778</td>\n      <td>0.055556</td>\n      <td>0.638889</td>\n      <td>0.194444</td>\n      <td>0.027778</td>\n      <td>0.027778</td>\n      <td>0.000000</td>\n      <td>[54, 35, 5, 54, 105, 35]</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>36.0</td>\n      <td>1.853903</td>\n      <td>0.611111</td>\n      <td>46.027778</td>\n      <td>6.0</td>\n      <td>0.027778</td>\n      <td>0.027778</td>\n      <td>0.055556</td>\n      <td>0.611111</td>\n      <td>0.222222</td>\n      <td>0.027778</td>\n      <td>0.027778</td>\n      <td>0.000000</td>\n      <td>[65, 42, 5, 65, 128, 42]</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>45.0</td>\n      <td>1.212873</td>\n      <td>0.600000</td>\n      <td>35.244444</td>\n      <td>6.0</td>\n      <td>0.022222</td>\n      <td>0.022222</td>\n      <td>0.044444</td>\n      <td>0.600000</td>\n      <td>0.266667</td>\n      <td>0.022222</td>\n      <td>0.022222</td>\n      <td>0.000000</td>\n      <td>[44, 28, 5, 44, 86, 28]</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:43.387797100Z",
     "start_time": "2023-10-12T09:59:43.345827400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "input_parameters = 13\n",
    "layer_size = 15\n",
    "layer_count = 6\n",
    "out_size = 6\n",
    "learning_rate = 0.007\n",
    "\n",
    "model = NeuralNet(input_parameters, layer_size, layer_count, out_size)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scorer = torchmetrics.regression.MeanSquaredError()\n",
    "\n",
    "trainer = NeuralNetTrainer(model, criterion, optimizer, scorer, 32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:44.108802500Z",
     "start_time": "2023-10-12T09:59:44.086807900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:44.708397200Z",
     "start_time": "2023-10-12T09:59:44.682457900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2         3         4         5         6  \\\n0  -0.112887 -0.490887 -0.788239 -0.442362  0.431484 -0.317028 -0.265376   \n1   0.360907 -0.848211 -0.446754 -0.887473  0.431484 -0.510602 -0.345482   \n2   0.031312  0.070136 -0.534586  0.073740  0.431484 -0.383668 -0.292954   \n3  -0.112887 -0.290599 -0.717587 -0.179584  0.431484 -0.317028 -0.265376   \n4  -1.534269  0.114494  1.437305  0.086492 -1.839484  1.425146  0.455577   \n..       ...       ...       ...       ...       ...       ...       ...   \n95  0.340308 -0.148231 -0.722228 -0.131279  0.431484 -0.503538 -0.342559   \n96  1.926488 -0.861015 -0.294334 -0.898630  0.431484 -0.854334 -0.487726   \n97 -1.740266  2.077890  1.859255  1.921419 -1.839484  2.231708  0.789352   \n98 -1.740266  3.294688  1.633561  3.375494 -1.839484  2.231708  0.789352   \n99 -1.554868  0.746136  1.543283  0.648576 -1.839484  1.489671  0.482279   \n\n           7         8         9        10        11        12  \\\n0  -0.194649 -0.236057  0.607487 -0.301440  0.587385  0.933373   \n1  -0.312215 -0.313941  0.307357  0.536177 -0.416768  0.491294   \n2  -0.235123 -0.033047  0.108582  0.173275  0.241693  0.781182   \n3  -0.194649 -0.177644  0.319362  0.057539  0.587385  0.933373   \n4   0.275613  1.487131 -1.013215 -1.557866 -0.416768 -1.719099   \n..       ...       ...       ...       ...       ...       ...   \n95 -0.307924 -0.396373  0.273619  0.754534 -0.380120  0.507429   \n96 -0.394621 -0.352610  0.632731  0.393592 -1.120613 -0.293705   \n97  0.602185  1.835987 -1.743532 -1.474769  0.977888 -1.719099   \n98  0.602185  1.649389 -1.513432 -1.474769  0.977888 -1.719099   \n99  0.301739  1.574750 -1.145273 -1.551218 -0.305195 -1.719099   \n\n                       label  \n0    [42, 27, 5, 42, 81, 27]  \n1    [47, 31, 5, 47, 92, 31]  \n2   [68, 44, 5, 68, 133, 44]  \n3    [43, 28, 5, 43, 83, 28]  \n4    [40, 26, 5, 40, 78, 26]  \n..                       ...  \n95  [54, 35, 5, 54, 105, 35]  \n96  [58, 38, 5, 58, 113, 38]  \n97  [54, 35, 5, 54, 105, 35]  \n98  [65, 42, 5, 65, 128, 42]  \n99   [44, 28, 5, 44, 86, 28]  \n\n[100 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.112887</td>\n      <td>-0.490887</td>\n      <td>-0.788239</td>\n      <td>-0.442362</td>\n      <td>0.431484</td>\n      <td>-0.317028</td>\n      <td>-0.265376</td>\n      <td>-0.194649</td>\n      <td>-0.236057</td>\n      <td>0.607487</td>\n      <td>-0.301440</td>\n      <td>0.587385</td>\n      <td>0.933373</td>\n      <td>[42, 27, 5, 42, 81, 27]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.360907</td>\n      <td>-0.848211</td>\n      <td>-0.446754</td>\n      <td>-0.887473</td>\n      <td>0.431484</td>\n      <td>-0.510602</td>\n      <td>-0.345482</td>\n      <td>-0.312215</td>\n      <td>-0.313941</td>\n      <td>0.307357</td>\n      <td>0.536177</td>\n      <td>-0.416768</td>\n      <td>0.491294</td>\n      <td>[47, 31, 5, 47, 92, 31]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.031312</td>\n      <td>0.070136</td>\n      <td>-0.534586</td>\n      <td>0.073740</td>\n      <td>0.431484</td>\n      <td>-0.383668</td>\n      <td>-0.292954</td>\n      <td>-0.235123</td>\n      <td>-0.033047</td>\n      <td>0.108582</td>\n      <td>0.173275</td>\n      <td>0.241693</td>\n      <td>0.781182</td>\n      <td>[68, 44, 5, 68, 133, 44]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.112887</td>\n      <td>-0.290599</td>\n      <td>-0.717587</td>\n      <td>-0.179584</td>\n      <td>0.431484</td>\n      <td>-0.317028</td>\n      <td>-0.265376</td>\n      <td>-0.194649</td>\n      <td>-0.177644</td>\n      <td>0.319362</td>\n      <td>0.057539</td>\n      <td>0.587385</td>\n      <td>0.933373</td>\n      <td>[43, 28, 5, 43, 83, 28]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.534269</td>\n      <td>0.114494</td>\n      <td>1.437305</td>\n      <td>0.086492</td>\n      <td>-1.839484</td>\n      <td>1.425146</td>\n      <td>0.455577</td>\n      <td>0.275613</td>\n      <td>1.487131</td>\n      <td>-1.013215</td>\n      <td>-1.557866</td>\n      <td>-0.416768</td>\n      <td>-1.719099</td>\n      <td>[40, 26, 5, 40, 78, 26]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>0.340308</td>\n      <td>-0.148231</td>\n      <td>-0.722228</td>\n      <td>-0.131279</td>\n      <td>0.431484</td>\n      <td>-0.503538</td>\n      <td>-0.342559</td>\n      <td>-0.307924</td>\n      <td>-0.396373</td>\n      <td>0.273619</td>\n      <td>0.754534</td>\n      <td>-0.380120</td>\n      <td>0.507429</td>\n      <td>[54, 35, 5, 54, 105, 35]</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>1.926488</td>\n      <td>-0.861015</td>\n      <td>-0.294334</td>\n      <td>-0.898630</td>\n      <td>0.431484</td>\n      <td>-0.854334</td>\n      <td>-0.487726</td>\n      <td>-0.394621</td>\n      <td>-0.352610</td>\n      <td>0.632731</td>\n      <td>0.393592</td>\n      <td>-1.120613</td>\n      <td>-0.293705</td>\n      <td>[58, 38, 5, 58, 113, 38]</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>-1.740266</td>\n      <td>2.077890</td>\n      <td>1.859255</td>\n      <td>1.921419</td>\n      <td>-1.839484</td>\n      <td>2.231708</td>\n      <td>0.789352</td>\n      <td>0.602185</td>\n      <td>1.835987</td>\n      <td>-1.743532</td>\n      <td>-1.474769</td>\n      <td>0.977888</td>\n      <td>-1.719099</td>\n      <td>[54, 35, 5, 54, 105, 35]</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>-1.740266</td>\n      <td>3.294688</td>\n      <td>1.633561</td>\n      <td>3.375494</td>\n      <td>-1.839484</td>\n      <td>2.231708</td>\n      <td>0.789352</td>\n      <td>0.602185</td>\n      <td>1.649389</td>\n      <td>-1.513432</td>\n      <td>-1.474769</td>\n      <td>0.977888</td>\n      <td>-1.719099</td>\n      <td>[65, 42, 5, 65, 128, 42]</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>-1.554868</td>\n      <td>0.746136</td>\n      <td>1.543283</td>\n      <td>0.648576</td>\n      <td>-1.839484</td>\n      <td>1.489671</td>\n      <td>0.482279</td>\n      <td>0.301739</td>\n      <td>1.574750</td>\n      <td>-1.145273</td>\n      <td>-1.551218</td>\n      <td>-0.305195</td>\n      <td>-1.719099</td>\n      <td>[44, 28, 5, 44, 86, 28]</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_dataset.drop(columns=['label']))\n",
    "scaled_dataset = scaler.transform(train_dataset.drop(columns=['label']))\n",
    "scaled_dataset = pd.DataFrame(scaled_dataset, columns=train_dataset.drop(columns=['label']).columns)\n",
    "train_dataset = pd.concat([scaled_dataset, train_dataset['label']], axis=1)\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:44:26.681761400Z",
     "start_time": "2023-10-12T09:44:26.615925300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_dataset.drop(columns=['label']), train_dataset['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:47.178209200Z",
     "start_time": "2023-10-12T09:59:47.156202200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Average Loss: 3283.8860\n",
      "Epoch [2/100], Average Loss: 3272.4633\n",
      "Epoch [3/100], Average Loss: 3245.6322\n",
      "Epoch [4/100], Average Loss: 3201.6300\n",
      "Epoch [5/100], Average Loss: 3110.5658\n",
      "Epoch [6/100], Average Loss: 2905.2577\n",
      "Epoch [7/100], Average Loss: 2548.3596\n",
      "Epoch [8/100], Average Loss: 1968.7076\n",
      "Epoch [9/100], Average Loss: 1349.8068\n",
      "Epoch [10/100], Average Loss: 1327.9887\n",
      "Epoch [11/100], Average Loss: 1044.6762\n",
      "Epoch [12/100], Average Loss: 683.7042\n",
      "Epoch [13/100], Average Loss: 578.4877\n",
      "Epoch [14/100], Average Loss: 487.8139\n",
      "Epoch [15/100], Average Loss: 327.6452\n",
      "Epoch [16/100], Average Loss: 278.5255\n",
      "Epoch [17/100], Average Loss: 303.3887\n",
      "Epoch [18/100], Average Loss: 227.5749\n",
      "Epoch [19/100], Average Loss: 208.6012\n",
      "Epoch [20/100], Average Loss: 229.7920\n",
      "Epoch [21/100], Average Loss: 206.8055\n",
      "Epoch [22/100], Average Loss: 180.7632\n",
      "Epoch [23/100], Average Loss: 199.0041\n",
      "Epoch [24/100], Average Loss: 171.3696\n",
      "Epoch [25/100], Average Loss: 144.6870\n",
      "Epoch [26/100], Average Loss: 142.9487\n",
      "Epoch [27/100], Average Loss: 123.9032\n",
      "Epoch [28/100], Average Loss: 111.6309\n",
      "Epoch [29/100], Average Loss: 106.1240\n",
      "Epoch [30/100], Average Loss: 97.5823\n",
      "Epoch [31/100], Average Loss: 95.7639\n",
      "Epoch [32/100], Average Loss: 93.3913\n",
      "Epoch [33/100], Average Loss: 95.1607\n",
      "Epoch [34/100], Average Loss: 91.9876\n",
      "Epoch [35/100], Average Loss: 90.6432\n",
      "Epoch [36/100], Average Loss: 90.4886\n",
      "Epoch [37/100], Average Loss: 92.3384\n",
      "Epoch [38/100], Average Loss: 87.2768\n",
      "Epoch [39/100], Average Loss: 90.0375\n",
      "Epoch [40/100], Average Loss: 91.7478\n",
      "Epoch [41/100], Average Loss: 88.9546\n",
      "Epoch [42/100], Average Loss: 90.8765\n",
      "Epoch [43/100], Average Loss: 88.0083\n",
      "Epoch [44/100], Average Loss: 88.3283\n",
      "Epoch [45/100], Average Loss: 92.1074\n",
      "Epoch [46/100], Average Loss: 91.0220\n",
      "Epoch [47/100], Average Loss: 90.5594\n",
      "Epoch [48/100], Average Loss: 87.8074\n",
      "Epoch [49/100], Average Loss: 86.0207\n",
      "Epoch [50/100], Average Loss: 85.9351\n",
      "Epoch [51/100], Average Loss: 86.8132\n",
      "Epoch [52/100], Average Loss: 88.4951\n",
      "Epoch [53/100], Average Loss: 90.8566\n",
      "Epoch [54/100], Average Loss: 87.7945\n",
      "Epoch [55/100], Average Loss: 86.1459\n",
      "Epoch [56/100], Average Loss: 93.4661\n",
      "Epoch [57/100], Average Loss: 90.9675\n",
      "Epoch [58/100], Average Loss: 90.0458\n",
      "Epoch [59/100], Average Loss: 85.8310\n",
      "Epoch [60/100], Average Loss: 84.8324\n",
      "Epoch [61/100], Average Loss: 90.7246\n",
      "Epoch [62/100], Average Loss: 85.2464\n",
      "Epoch [63/100], Average Loss: 87.8605\n",
      "Epoch [64/100], Average Loss: 86.8234\n",
      "Epoch [65/100], Average Loss: 86.4624\n",
      "Epoch [66/100], Average Loss: 85.9011\n",
      "Epoch [67/100], Average Loss: 85.7898\n",
      "Epoch [68/100], Average Loss: 85.0147\n",
      "Epoch [69/100], Average Loss: 86.8615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 12:59:47,755\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Average Loss: 86.3832\n",
      "Epoch [71/100], Average Loss: 84.9662\n",
      "Epoch [72/100], Average Loss: 85.7414\n",
      "Epoch [73/100], Average Loss: 85.3628\n",
      "Epoch [74/100], Average Loss: 85.5347\n",
      "Epoch [75/100], Average Loss: 85.6914\n",
      "Epoch [76/100], Average Loss: 85.3526\n",
      "Epoch [77/100], Average Loss: 85.4710\n",
      "Epoch [78/100], Average Loss: 84.2442\n",
      "Epoch [79/100], Average Loss: 84.4181\n",
      "Epoch [80/100], Average Loss: 87.8093\n",
      "Epoch [81/100], Average Loss: 85.4733\n",
      "Epoch [82/100], Average Loss: 84.3529\n",
      "Epoch [83/100], Average Loss: 92.7867\n",
      "Epoch [84/100], Average Loss: 85.8525\n",
      "Epoch [85/100], Average Loss: 84.5843\n",
      "Epoch [86/100], Average Loss: 87.5026\n",
      "Epoch [87/100], Average Loss: 84.8398\n",
      "Epoch [88/100], Average Loss: 83.3778\n",
      "Epoch [89/100], Average Loss: 84.3621\n",
      "Epoch [90/100], Average Loss: 86.1120\n",
      "Epoch [91/100], Average Loss: 86.8025\n",
      "Epoch [92/100], Average Loss: 90.1473\n",
      "Epoch [93/100], Average Loss: 86.2607\n",
      "Epoch [94/100], Average Loss: 86.5683\n",
      "Epoch [95/100], Average Loss: 87.4755\n",
      "Epoch [96/100], Average Loss: 83.9455\n",
      "Epoch [97/100], Average Loss: 86.6196\n",
      "Epoch [98/100], Average Loss: 83.4276\n",
      "Epoch [99/100], Average Loss: 91.9074\n",
      "Epoch [100/100], Average Loss: 83.1348\n",
      "Epoch [1/100], Average Loss: 83.4292\n",
      "Epoch [2/100], Average Loss: 85.9608\n",
      "Epoch [3/100], Average Loss: 84.3592\n",
      "Epoch [4/100], Average Loss: 83.8072\n",
      "Epoch [5/100], Average Loss: 81.1154\n",
      "Epoch [6/100], Average Loss: 84.8601\n",
      "Epoch [7/100], Average Loss: 82.2718\n",
      "Epoch [8/100], Average Loss: 83.5598\n",
      "Epoch [9/100], Average Loss: 80.0121\n",
      "Epoch [10/100], Average Loss: 82.1382\n",
      "Epoch [11/100], Average Loss: 80.6128\n",
      "Epoch [12/100], Average Loss: 81.1544\n",
      "Epoch [13/100], Average Loss: 81.0517\n",
      "Epoch [14/100], Average Loss: 79.8199\n",
      "Epoch [15/100], Average Loss: 82.3207\n",
      "Epoch [16/100], Average Loss: 81.5735\n",
      "Epoch [17/100], Average Loss: 81.9015\n",
      "Epoch [18/100], Average Loss: 80.1041\n",
      "Epoch [19/100], Average Loss: 82.7437\n",
      "Epoch [20/100], Average Loss: 82.7200\n",
      "Epoch [21/100], Average Loss: 80.1363\n",
      "Epoch [22/100], Average Loss: 81.2840\n",
      "Epoch [23/100], Average Loss: 80.6025\n",
      "Epoch [24/100], Average Loss: 85.0016\n",
      "Epoch [25/100], Average Loss: 80.5284\n",
      "Epoch [26/100], Average Loss: 82.6734\n",
      "Epoch [27/100], Average Loss: 80.1160\n",
      "Epoch [28/100], Average Loss: 81.2254\n",
      "Epoch [29/100], Average Loss: 80.3292\n",
      "Epoch [30/100], Average Loss: 80.6736\n",
      "Epoch [31/100], Average Loss: 79.0389\n",
      "Epoch [32/100], Average Loss: 79.7190\n",
      "Epoch [33/100], Average Loss: 79.2933\n",
      "Epoch [34/100], Average Loss: 81.9928\n",
      "Epoch [35/100], Average Loss: 82.1303\n",
      "Epoch [36/100], Average Loss: 79.8729\n",
      "Epoch [37/100], Average Loss: 78.5878\n",
      "Epoch [38/100], Average Loss: 84.3753\n",
      "Epoch [39/100], Average Loss: 81.2494\n",
      "Epoch [40/100], Average Loss: 84.3491\n",
      "Epoch [41/100], Average Loss: 78.7513\n",
      "Epoch [42/100], Average Loss: 85.1440\n",
      "Epoch [43/100], Average Loss: 79.0127\n",
      "Epoch [44/100], Average Loss: 87.3399\n",
      "Epoch [45/100], Average Loss: 77.1082\n",
      "Epoch [46/100], Average Loss: 85.0349\n",
      "Epoch [47/100], Average Loss: 78.4700\n",
      "Epoch [48/100], Average Loss: 81.4279\n",
      "Epoch [49/100], Average Loss: 79.0753\n",
      "Epoch [50/100], Average Loss: 82.9240\n",
      "Epoch [51/100], Average Loss: 78.3463\n",
      "Epoch [52/100], Average Loss: 81.1409\n",
      "Epoch [53/100], Average Loss: 77.9498\n",
      "Epoch [54/100], Average Loss: 84.8846\n",
      "Epoch [55/100], Average Loss: 77.3537\n",
      "Epoch [56/100], Average Loss: 78.7919\n",
      "Epoch [57/100], Average Loss: 77.0972\n",
      "Epoch [58/100], Average Loss: 78.7574\n",
      "Epoch [59/100], Average Loss: 77.5731\n",
      "Epoch [60/100], Average Loss: 77.9008\n",
      "Epoch [61/100], Average Loss: 78.3422\n",
      "Epoch [62/100], Average Loss: 78.1139\n",
      "Epoch [63/100], Average Loss: 77.3653\n",
      "Epoch [64/100], Average Loss: 83.2000\n",
      "Epoch [65/100], Average Loss: 76.0178\n",
      "Epoch [66/100], Average Loss: 80.9277\n",
      "Epoch [67/100], Average Loss: 77.4288\n",
      "Epoch [68/100], Average Loss: 77.3472\n",
      "Epoch [69/100], Average Loss: 77.3177\n",
      "Epoch [70/100], Average Loss: 78.2071\n",
      "Epoch [71/100], Average Loss: 84.6774\n",
      "Epoch [72/100], Average Loss: 77.7248\n",
      "Epoch [73/100], Average Loss: 76.8848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 12:59:48,163\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Average Loss: 77.3472\n",
      "Epoch [75/100], Average Loss: 74.8206\n",
      "Epoch [76/100], Average Loss: 78.9359\n",
      "Epoch [77/100], Average Loss: 75.7585\n",
      "Epoch [78/100], Average Loss: 76.3189\n",
      "Epoch [79/100], Average Loss: 74.6604\n",
      "Epoch [80/100], Average Loss: 77.6995\n",
      "Epoch [81/100], Average Loss: 83.3102\n",
      "Epoch [82/100], Average Loss: 79.0176\n",
      "Epoch [83/100], Average Loss: 75.7706\n",
      "Epoch [84/100], Average Loss: 82.0960\n",
      "Epoch [85/100], Average Loss: 73.7336\n",
      "Epoch [86/100], Average Loss: 81.9187\n",
      "Epoch [87/100], Average Loss: 75.7803\n",
      "Epoch [88/100], Average Loss: 78.5866\n",
      "Epoch [89/100], Average Loss: 77.6544\n",
      "Epoch [90/100], Average Loss: 75.3214\n",
      "Epoch [91/100], Average Loss: 90.6884\n",
      "Epoch [92/100], Average Loss: 82.6620\n",
      "Epoch [93/100], Average Loss: 85.3454\n",
      "Epoch [94/100], Average Loss: 82.2631\n",
      "Epoch [95/100], Average Loss: 72.6090\n",
      "Epoch [96/100], Average Loss: 84.5086\n",
      "Epoch [97/100], Average Loss: 71.9454\n",
      "Epoch [98/100], Average Loss: 83.8803\n",
      "Epoch [99/100], Average Loss: 75.0441\n",
      "Epoch [100/100], Average Loss: 80.1552\n",
      "Epoch [1/100], Average Loss: 83.1535\n",
      "Epoch [2/100], Average Loss: 80.9196\n",
      "Epoch [3/100], Average Loss: 82.2301\n",
      "Epoch [4/100], Average Loss: 80.0374\n",
      "Epoch [5/100], Average Loss: 82.7992\n",
      "Epoch [6/100], Average Loss: 78.3350\n",
      "Epoch [7/100], Average Loss: 89.1123\n",
      "Epoch [8/100], Average Loss: 82.0033\n",
      "Epoch [9/100], Average Loss: 87.3262\n",
      "Epoch [10/100], Average Loss: 85.9286\n",
      "Epoch [11/100], Average Loss: 85.0350\n",
      "Epoch [12/100], Average Loss: 88.7704\n",
      "Epoch [13/100], Average Loss: 78.0616\n",
      "Epoch [14/100], Average Loss: 93.9646\n",
      "Epoch [15/100], Average Loss: 80.7052\n",
      "Epoch [16/100], Average Loss: 90.5238\n",
      "Epoch [17/100], Average Loss: 81.9693\n",
      "Epoch [18/100], Average Loss: 91.4711\n",
      "Epoch [19/100], Average Loss: 79.4675\n",
      "Epoch [20/100], Average Loss: 78.4520\n",
      "Epoch [21/100], Average Loss: 78.0754\n",
      "Epoch [22/100], Average Loss: 77.4309\n",
      "Epoch [23/100], Average Loss: 77.0476\n",
      "Epoch [24/100], Average Loss: 76.4389\n",
      "Epoch [25/100], Average Loss: 78.4989\n",
      "Epoch [26/100], Average Loss: 80.9125\n",
      "Epoch [27/100], Average Loss: 79.1740\n",
      "Epoch [28/100], Average Loss: 77.9900\n",
      "Epoch [29/100], Average Loss: 78.5683\n",
      "Epoch [30/100], Average Loss: 74.9309\n",
      "Epoch [31/100], Average Loss: 77.2214\n",
      "Epoch [32/100], Average Loss: 74.1692\n",
      "Epoch [33/100], Average Loss: 77.2610\n",
      "Epoch [34/100], Average Loss: 77.7293\n",
      "Epoch [35/100], Average Loss: 75.6710\n",
      "Epoch [36/100], Average Loss: 82.8668\n",
      "Epoch [37/100], Average Loss: 71.6244\n",
      "Epoch [38/100], Average Loss: 80.4022\n",
      "Epoch [39/100], Average Loss: 76.3206\n",
      "Epoch [40/100], Average Loss: 77.5815\n",
      "Epoch [41/100], Average Loss: 73.3515\n",
      "Epoch [42/100], Average Loss: 70.7757\n",
      "Epoch [43/100], Average Loss: 82.1684\n",
      "Epoch [44/100], Average Loss: 72.8902\n",
      "Epoch [45/100], Average Loss: 79.0653\n",
      "Epoch [46/100], Average Loss: 69.2437\n",
      "Epoch [47/100], Average Loss: 77.7392\n",
      "Epoch [48/100], Average Loss: 73.5184\n",
      "Epoch [49/100], Average Loss: 70.4081\n",
      "Epoch [50/100], Average Loss: 70.3483\n",
      "Epoch [51/100], Average Loss: 68.8927\n",
      "Epoch [52/100], Average Loss: 69.8147\n",
      "Epoch [53/100], Average Loss: 71.2441\n",
      "Epoch [54/100], Average Loss: 79.1534\n",
      "Epoch [55/100], Average Loss: 76.8820\n",
      "Epoch [56/100], Average Loss: 78.7497\n",
      "Epoch [57/100], Average Loss: 70.8417\n",
      "Epoch [58/100], Average Loss: 69.3531\n",
      "Epoch [59/100], Average Loss: 71.3273\n",
      "Epoch [60/100], Average Loss: 67.4334\n",
      "Epoch [61/100], Average Loss: 68.7014\n",
      "Epoch [62/100], Average Loss: 72.4379\n",
      "Epoch [63/100], Average Loss: 65.5262\n",
      "Epoch [64/100], Average Loss: 73.0639\n",
      "Epoch [65/100], Average Loss: 74.9674\n",
      "Epoch [66/100], Average Loss: 70.0497\n",
      "Epoch [67/100], Average Loss: 73.4290\n",
      "Epoch [68/100], Average Loss: 71.7079\n",
      "Epoch [69/100], Average Loss: 68.6595\n",
      "Epoch [70/100], Average Loss: 74.8422\n",
      "Epoch [71/100], Average Loss: 71.0913\n",
      "Epoch [72/100], Average Loss: 67.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 12:59:48,566\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100], Average Loss: 84.6776\n",
      "Epoch [74/100], Average Loss: 77.8772\n",
      "Epoch [75/100], Average Loss: 74.4968\n",
      "Epoch [76/100], Average Loss: 76.6297\n",
      "Epoch [77/100], Average Loss: 60.3831\n",
      "Epoch [78/100], Average Loss: 73.2085\n",
      "Epoch [79/100], Average Loss: 64.2968\n",
      "Epoch [80/100], Average Loss: 64.2377\n",
      "Epoch [81/100], Average Loss: 69.4933\n",
      "Epoch [82/100], Average Loss: 59.8596\n",
      "Epoch [83/100], Average Loss: 66.6536\n",
      "Epoch [84/100], Average Loss: 65.0913\n",
      "Epoch [85/100], Average Loss: 60.0239\n",
      "Epoch [86/100], Average Loss: 71.7974\n",
      "Epoch [87/100], Average Loss: 78.6015\n",
      "Epoch [88/100], Average Loss: 61.3856\n",
      "Epoch [89/100], Average Loss: 71.3982\n",
      "Epoch [90/100], Average Loss: 57.5129\n",
      "Epoch [91/100], Average Loss: 62.8964\n",
      "Epoch [92/100], Average Loss: 57.0701\n",
      "Epoch [93/100], Average Loss: 57.9866\n",
      "Epoch [94/100], Average Loss: 56.4372\n",
      "Epoch [95/100], Average Loss: 63.8218\n",
      "Epoch [96/100], Average Loss: 57.5541\n",
      "Epoch [97/100], Average Loss: 58.4155\n",
      "Epoch [98/100], Average Loss: 58.0937\n",
      "Epoch [99/100], Average Loss: 57.1783\n",
      "Epoch [100/100], Average Loss: 54.8117\n",
      "Epoch [1/100], Average Loss: 64.1904\n",
      "Epoch [2/100], Average Loss: 62.5441\n",
      "Epoch [3/100], Average Loss: 59.7381\n",
      "Epoch [4/100], Average Loss: 59.7735\n",
      "Epoch [5/100], Average Loss: 65.9963\n",
      "Epoch [6/100], Average Loss: 82.7819\n",
      "Epoch [7/100], Average Loss: 75.4303\n",
      "Epoch [8/100], Average Loss: 67.7228\n",
      "Epoch [9/100], Average Loss: 68.9815\n",
      "Epoch [10/100], Average Loss: 72.3216\n",
      "Epoch [11/100], Average Loss: 62.5037\n",
      "Epoch [12/100], Average Loss: 57.8547\n",
      "Epoch [13/100], Average Loss: 75.0221\n",
      "Epoch [14/100], Average Loss: 65.3917\n",
      "Epoch [15/100], Average Loss: 63.4729\n",
      "Epoch [16/100], Average Loss: 64.1004\n",
      "Epoch [17/100], Average Loss: 57.9346\n",
      "Epoch [18/100], Average Loss: 61.1127\n",
      "Epoch [19/100], Average Loss: 56.5506\n",
      "Epoch [20/100], Average Loss: 60.6806\n",
      "Epoch [21/100], Average Loss: 60.5337\n",
      "Epoch [22/100], Average Loss: 55.4137\n",
      "Epoch [23/100], Average Loss: 54.0323\n",
      "Epoch [24/100], Average Loss: 55.1292\n",
      "Epoch [25/100], Average Loss: 57.9639\n",
      "Epoch [26/100], Average Loss: 53.5064\n",
      "Epoch [27/100], Average Loss: 52.4049\n",
      "Epoch [28/100], Average Loss: 55.9443\n",
      "Epoch [29/100], Average Loss: 57.6302\n",
      "Epoch [30/100], Average Loss: 52.5989\n",
      "Epoch [31/100], Average Loss: 52.3455\n",
      "Epoch [32/100], Average Loss: 59.6136\n",
      "Epoch [33/100], Average Loss: 65.1102\n",
      "Epoch [34/100], Average Loss: 52.3472\n",
      "Epoch [35/100], Average Loss: 49.6613\n",
      "Epoch [36/100], Average Loss: 62.2567\n",
      "Epoch [37/100], Average Loss: 55.4226\n",
      "Epoch [38/100], Average Loss: 52.5235\n",
      "Epoch [39/100], Average Loss: 53.9167\n",
      "Epoch [40/100], Average Loss: 49.7495\n",
      "Epoch [41/100], Average Loss: 57.8953\n",
      "Epoch [42/100], Average Loss: 50.2116\n",
      "Epoch [43/100], Average Loss: 47.5564\n",
      "Epoch [44/100], Average Loss: 52.3756\n",
      "Epoch [45/100], Average Loss: 66.1215\n",
      "Epoch [46/100], Average Loss: 61.6453\n",
      "Epoch [47/100], Average Loss: 63.7599\n",
      "Epoch [48/100], Average Loss: 81.5212\n",
      "Epoch [49/100], Average Loss: 58.3755\n",
      "Epoch [50/100], Average Loss: 60.1166\n",
      "Epoch [51/100], Average Loss: 95.9003\n",
      "Epoch [52/100], Average Loss: 96.1311\n",
      "Epoch [53/100], Average Loss: 62.2521\n",
      "Epoch [54/100], Average Loss: 66.0820\n",
      "Epoch [55/100], Average Loss: 49.9442\n",
      "Epoch [56/100], Average Loss: 55.4210\n",
      "Epoch [57/100], Average Loss: 53.1780\n",
      "Epoch [58/100], Average Loss: 49.2308\n",
      "Epoch [59/100], Average Loss: 52.4548\n",
      "Epoch [60/100], Average Loss: 60.9132\n",
      "Epoch [61/100], Average Loss: 66.1943\n",
      "Epoch [62/100], Average Loss: 65.3631\n",
      "Epoch [63/100], Average Loss: 55.0181\n",
      "Epoch [64/100], Average Loss: 52.5049\n",
      "Epoch [65/100], Average Loss: 73.3912\n",
      "Epoch [66/100], Average Loss: 87.1514\n",
      "Epoch [67/100], Average Loss: 54.8927\n",
      "Epoch [68/100], Average Loss: 81.4357\n",
      "Epoch [69/100], Average Loss: 60.7933\n",
      "Epoch [70/100], Average Loss: 51.7938\n",
      "Epoch [71/100], Average Loss: 48.4769\n",
      "Epoch [72/100], Average Loss: 54.2784\n",
      "Epoch [73/100], Average Loss: 56.5718\n",
      "Epoch [74/100], Average Loss: 50.8912\n",
      "Epoch [75/100], Average Loss: 53.2010\n",
      "Epoch [76/100], Average Loss: 50.9050\n",
      "Epoch [77/100], Average Loss: 48.9104\n",
      "Epoch [78/100], Average Loss: 47.6960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 12:59:48,988\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100], Average Loss: 52.7594\n",
      "Epoch [80/100], Average Loss: 55.8416\n",
      "Epoch [81/100], Average Loss: 45.7440\n",
      "Epoch [82/100], Average Loss: 53.4095\n",
      "Epoch [83/100], Average Loss: 50.3921\n",
      "Epoch [84/100], Average Loss: 51.8713\n",
      "Epoch [85/100], Average Loss: 56.2956\n",
      "Epoch [86/100], Average Loss: 60.7181\n",
      "Epoch [87/100], Average Loss: 48.3878\n",
      "Epoch [88/100], Average Loss: 58.6028\n",
      "Epoch [89/100], Average Loss: 50.9777\n",
      "Epoch [90/100], Average Loss: 48.2478\n",
      "Epoch [91/100], Average Loss: 48.5466\n",
      "Epoch [92/100], Average Loss: 49.3376\n",
      "Epoch [93/100], Average Loss: 54.6049\n",
      "Epoch [94/100], Average Loss: 45.1911\n",
      "Epoch [95/100], Average Loss: 50.3543\n",
      "Epoch [96/100], Average Loss: 49.6365\n",
      "Epoch [97/100], Average Loss: 51.7078\n",
      "Epoch [98/100], Average Loss: 63.7571\n",
      "Epoch [99/100], Average Loss: 72.0100\n",
      "Epoch [100/100], Average Loss: 61.1766\n",
      "Epoch [1/100], Average Loss: 53.9199\n",
      "Epoch [2/100], Average Loss: 60.6693\n",
      "Epoch [3/100], Average Loss: 62.4540\n",
      "Epoch [4/100], Average Loss: 67.7065\n",
      "Epoch [5/100], Average Loss: 46.4094\n",
      "Epoch [6/100], Average Loss: 66.5011\n",
      "Epoch [7/100], Average Loss: 60.9866\n",
      "Epoch [8/100], Average Loss: 70.1288\n",
      "Epoch [9/100], Average Loss: 52.6870\n",
      "Epoch [10/100], Average Loss: 55.8281\n",
      "Epoch [11/100], Average Loss: 47.5131\n",
      "Epoch [12/100], Average Loss: 46.8101\n",
      "Epoch [13/100], Average Loss: 50.0526\n",
      "Epoch [14/100], Average Loss: 48.7649\n",
      "Epoch [15/100], Average Loss: 47.1266\n",
      "Epoch [16/100], Average Loss: 49.1416\n",
      "Epoch [17/100], Average Loss: 52.3465\n",
      "Epoch [18/100], Average Loss: 48.1320\n",
      "Epoch [19/100], Average Loss: 51.7493\n",
      "Epoch [20/100], Average Loss: 48.7420\n",
      "Epoch [21/100], Average Loss: 51.6137\n",
      "Epoch [22/100], Average Loss: 48.5586\n",
      "Epoch [23/100], Average Loss: 47.5838\n",
      "Epoch [24/100], Average Loss: 48.9946\n",
      "Epoch [25/100], Average Loss: 45.3689\n",
      "Epoch [26/100], Average Loss: 48.5099\n",
      "Epoch [27/100], Average Loss: 47.3923\n",
      "Epoch [28/100], Average Loss: 45.0336\n",
      "Epoch [29/100], Average Loss: 47.0513\n",
      "Epoch [30/100], Average Loss: 47.6137\n",
      "Epoch [31/100], Average Loss: 44.3986\n",
      "Epoch [32/100], Average Loss: 47.9898\n",
      "Epoch [33/100], Average Loss: 51.9826\n",
      "Epoch [34/100], Average Loss: 45.4641\n",
      "Epoch [35/100], Average Loss: 53.8168\n",
      "Epoch [36/100], Average Loss: 52.1206\n",
      "Epoch [37/100], Average Loss: 46.6955\n",
      "Epoch [38/100], Average Loss: 52.1594\n",
      "Epoch [39/100], Average Loss: 46.9365\n",
      "Epoch [40/100], Average Loss: 45.5323\n",
      "Epoch [41/100], Average Loss: 47.9769\n",
      "Epoch [42/100], Average Loss: 45.8698\n",
      "Epoch [43/100], Average Loss: 46.6860\n",
      "Epoch [44/100], Average Loss: 47.1246\n",
      "Epoch [45/100], Average Loss: 51.1709\n",
      "Epoch [46/100], Average Loss: 50.5560\n",
      "Epoch [47/100], Average Loss: 47.0513\n",
      "Epoch [48/100], Average Loss: 54.4415\n",
      "Epoch [49/100], Average Loss: 50.9172\n",
      "Epoch [50/100], Average Loss: 47.1958\n",
      "Epoch [51/100], Average Loss: 55.4812\n",
      "Epoch [52/100], Average Loss: 48.2938\n",
      "Epoch [53/100], Average Loss: 47.8445\n",
      "Epoch [54/100], Average Loss: 55.4035\n",
      "Epoch [55/100], Average Loss: 44.4951\n",
      "Epoch [56/100], Average Loss: 49.1666\n",
      "Epoch [57/100], Average Loss: 57.9999\n",
      "Epoch [58/100], Average Loss: 45.6191\n",
      "Epoch [59/100], Average Loss: 52.0540\n",
      "Epoch [60/100], Average Loss: 50.3780\n",
      "Epoch [61/100], Average Loss: 50.1691\n",
      "Epoch [62/100], Average Loss: 49.0722\n",
      "Epoch [63/100], Average Loss: 48.1794\n",
      "Epoch [64/100], Average Loss: 51.0017\n",
      "Epoch [65/100], Average Loss: 45.1309\n",
      "Epoch [66/100], Average Loss: 49.8522\n",
      "Epoch [67/100], Average Loss: 47.9433\n",
      "Epoch [68/100], Average Loss: 47.0707\n",
      "Epoch [69/100], Average Loss: 48.6673\n",
      "Epoch [70/100], Average Loss: 50.0196\n",
      "Epoch [71/100], Average Loss: 51.6306\n",
      "Epoch [72/100], Average Loss: 57.3519\n",
      "Epoch [73/100], Average Loss: 47.5611\n",
      "Epoch [74/100], Average Loss: 47.3742\n",
      "Epoch [75/100], Average Loss: 45.5194\n",
      "Epoch [76/100], Average Loss: 46.5878\n",
      "Epoch [77/100], Average Loss: 49.6573\n",
      "Epoch [78/100], Average Loss: 52.9918\n",
      "Epoch [79/100], Average Loss: 45.6360\n",
      "Epoch [80/100], Average Loss: 51.2085\n",
      "Epoch [81/100], Average Loss: 49.9632\n",
      "Epoch [82/100], Average Loss: 46.2771\n",
      "Epoch [83/100], Average Loss: 49.8393\n",
      "Epoch [84/100], Average Loss: 47.5238\n",
      "Epoch [85/100], Average Loss: 45.4485\n",
      "Epoch [86/100], Average Loss: 51.2099\n",
      "Epoch [87/100], Average Loss: 44.6143\n",
      "Epoch [88/100], Average Loss: 54.6777\n",
      "Epoch [89/100], Average Loss: 45.7972\n",
      "Epoch [90/100], Average Loss: 49.4257\n",
      "Epoch [91/100], Average Loss: 45.5030\n",
      "Epoch [92/100], Average Loss: 46.0314\n",
      "Epoch [93/100], Average Loss: 49.5661\n",
      "Epoch [94/100], Average Loss: 44.8192\n",
      "Epoch [95/100], Average Loss: 47.7959\n",
      "Epoch [96/100], Average Loss: 48.1798\n",
      "Epoch [97/100], Average Loss: 49.3471\n",
      "Epoch [98/100], Average Loss: 47.5718\n",
      "Epoch [99/100], Average Loss: 50.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 12:59:49,431\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Average Loss: 51.8744\n"
     ]
    }
   ],
   "source": [
    "from sampo.scheduler.selection.validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "s = cross_val_score(X=x_train,\n",
    "                    y=y_train,\n",
    "                    model=trainer,\n",
    "                    epochs=100,\n",
    "                    folds=5,\n",
    "                    shuffle=True,\n",
    "                    type_task=NeuralNetType.REGRESSION)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:49.447237Z",
     "start_time": "2023-10-12T09:59:47.332247300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:49.492360200Z",
     "start_time": "2023-10-12T09:59:49.449239800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2189, grad_fn=<SelectBackward0>)\n",
      "1 tensor(1.6328, grad_fn=<SelectBackward0>)\n",
      "2 tensor(2.8139, grad_fn=<SelectBackward0>)\n",
      "3 tensor(0.4015, grad_fn=<SelectBackward0>)\n",
      "4 tensor(0.8049, grad_fn=<SelectBackward0>)\n",
      "5 tensor(3.7189, grad_fn=<SelectBackward0>)\n",
      "6 tensor(1.6510, grad_fn=<SelectBackward0>)\n",
      "7 tensor(1.9515, grad_fn=<SelectBackward0>)\n",
      "8 tensor(2.3776, grad_fn=<SelectBackward0>)\n",
      "9 tensor(0.3425, grad_fn=<SelectBackward0>)\n",
      "10 tensor(0.9870, grad_fn=<SelectBackward0>)\n",
      "11 tensor(1.2404, grad_fn=<SelectBackward0>)\n",
      "12 tensor(1.6748, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "index = [-10000] * input_parameters\n",
    "for fold in list(model.parameters())[0]:\n",
    "    for i in range(len(fold)):\n",
    "        index[i] = max(index[i], fold[i])\n",
    "for i in range(len(index)):\n",
    "    print(i, index[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:49.724459500Z",
     "start_time": "2023-10-12T09:59:49.693420600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239.33650716145834, 7.479265848795573, <sampo.scheduler.selection.neural_net.NeuralNetTrainer object at 0x000001B47F467CA0>)\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:50.269318100Z",
     "start_time": "2023-10-12T09:59:50.227317600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "test_dataset = x_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:50.823267800Z",
     "start_time": "2023-10-12T09:59:50.786268700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:51.396792200Z",
     "start_time": "2023-10-12T09:59:51.377752700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "15.86457572932663"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = trainer.predict([torch.Tensor(v) for v in test_dataset.iloc[:, :].values])\n",
    "array = []\n",
    "label_test = y_test.to_numpy()\n",
    "for i in range(len(predicted)):\n",
    "    array.append(sum((predicted[i] - label_test[i]) ** 2))\n",
    "(sum(array) / len(array)) ** (1 / 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:51.741333Z",
     "start_time": "2023-10-12T09:59:51.716272200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 57.3298   ,  37.07157  ,   5.1664343,  57.324955 , 111.55159  ,\n         37.04023  ],\n       [ 61.66734  ,  39.87013  ,   5.520999 ,  61.64526  , 119.96209  ,\n         39.843422 ],\n       [ 56.182514 ,  36.330105 ,   5.0719843,  56.181637 , 109.32636  ,\n         36.2983   ],\n       [ 60.230453 ,  38.932423 ,   5.397867 ,  60.209064 , 117.17043  ,\n         38.910656 ],\n       [ 54.911392 ,  35.518536 ,   4.9726486,  54.91959  , 106.866066 ,\n         35.480156 ],\n       [ 66.93181  ,  43.22555  ,   5.929315 ,  66.86941  , 130.14854  ,\n         43.229595 ],\n       [ 68.20225  ,  44.065914 ,   6.0442486,  68.14454  , 132.62263  ,\n         44.058666 ],\n       [ 51.52979  ,  33.341614 ,   4.698829 ,  51.553715 , 100.311646 ,\n         33.29665  ],\n       [ 65.51152  ,  42.340874 ,   5.8301754,  65.46968  , 127.411    ,\n         42.32404  ],\n       [ 48.04828  ,  31.110708 ,   4.4224205,  48.093285 ,  93.56901  ,\n         31.052694 ],\n       [ 59.28066  ,  38.324837 ,   5.3230095,  59.265507 , 115.331505 ,\n         38.29889  ],\n       [ 50.916935 ,  32.9493   ,   4.6503797,  50.944763 ,  99.12496  ,\n         32.90181  ],\n       [ 55.115025 ,  35.64617  ,   4.98728  ,  55.120667 , 107.259026 ,\n         35.61032  ],\n       [ 25.063648 ,  16.318468 ,   2.5634387,  25.21761  ,  49.021267 ,\n         16.213272 ],\n       [ 51.452763 ,  33.296448 ,   4.6949477,  51.479134 , 100.164665 ,\n         33.24865  ],\n       [ 68.34338  ,  44.160736 ,   6.057791 ,  68.28689  , 132.89824  ,\n         44.15135  ],\n       [ 56.53661  ,  36.561203 ,   5.102318 ,  56.535595 , 110.01437  ,\n         36.528206 ],\n       [ 49.244194 ,  31.875542 ,   4.516568 ,  49.281246 ,  95.88435  ,\n         31.82292  ],\n       [ 70.96354  ,  45.852276 ,   6.2725115,  70.897125 , 137.9793   ,\n         45.84507  ],\n       [ 55.7234   ,  36.02219  ,   5.0281973,  55.718815 , 108.43002  ,\n         35.997025 ],\n       [ 50.985256 ,  32.99373  ,   4.656131 ,  51.012997 ,  99.257675 ,\n         32.94613  ],\n       [ 49.71913  ,  32.182137 ,   4.5554767,  49.754387 ,  96.80537  ,\n         32.129932 ],\n       [ 53.0678   ,  34.334606 ,   4.8249073,  53.085926 , 103.29421  ,\n         34.29088  ],\n       [ 60.43234  ,  39.065845 ,   5.4160686,  60.411625 , 117.5635   ,\n         39.04235  ],\n       [ 51.14066  ,  33.094048 ,   4.6688766,  51.167797 ,  99.559    ,\n         33.04656  ]], dtype=float32)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:21:23.171674Z",
     "start_time": "2023-10-12T14:21:23.141658900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([list([63, 41, 5, 63, 124, 41]), list([59, 38, 5, 59, 116, 38]),\n       list([57, 37, 5, 57, 112, 37]), list([61, 40, 5, 61, 120, 40]),\n       list([56, 36, 5, 56, 110, 36]), list([56, 36, 5, 56, 109, 36]),\n       list([66, 43, 5, 66, 129, 43]), list([58, 38, 5, 58, 113, 38]),\n       list([53, 34, 5, 53, 104, 34]), list([40, 26, 5, 40, 77, 26]),\n       list([57, 37, 5, 57, 111, 37]), list([58, 37, 5, 58, 113, 37]),\n       list([50, 32, 5, 50, 97, 32]), list([31, 20, 5, 31, 60, 20]),\n       list([53, 34, 5, 53, 102, 34]), list([65, 42, 5, 65, 126, 42]),\n       list([65, 42, 5, 65, 127, 42]), list([48, 31, 5, 48, 92, 31]),\n       list([65, 42, 5, 65, 128, 42]), list([49, 32, 5, 49, 96, 32]),\n       list([44, 28, 5, 44, 85, 28]), list([47, 31, 5, 47, 92, 31]),\n       list([41, 26, 5, 41, 80, 26]), list([55, 36, 5, 55, 108, 36]),\n       list([49, 32, 5, 49, 95, 32])], dtype=object)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:59:53.023560500Z",
     "start_time": "2023-10-12T09:59:52.997563200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
