{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:11:17.843586100Z",
     "start_time": "2023-10-12T14:11:15.156587300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sampo.scheduler.selection.neural_net import NeuralNetTrainer, NeuralNet, NeuralNetType\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'datasets/dataset_1000_objs.csv')\n",
    "train_dataset = pd.read_csv(path, index_col='index')\n",
    "for col in train_dataset.columns[:-1]:\n",
    "    train_dataset[col] = train_dataset[col].apply(lambda x: float(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:11:19.539589300Z",
     "start_time": "2023-10-12T14:11:17.846586500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "input_parameters = 13\n",
    "layer_size = 15\n",
    "layer_count = 6\n",
    "classification_size = 2\n",
    "learning_rate = 0.007\n",
    "\n",
    "model = NeuralNet(input_parameters, layer_size, layer_count, classification_size, task_type=NeuralNetType.CLASSIFICATION)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scorer = torchmetrics.classification.BinaryAccuracy()\n",
    "\n",
    "trainer = NeuralNetTrainer(model, criterion, optimizer, scorer, 32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:11:19.555588600Z",
     "start_time": "2023-10-12T14:11:19.542589600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "pandas.core.frame.DataFrame"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:11:19.574585500Z",
     "start_time": "2023-10-12T14:11:19.558590Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n0    -0.674765 -0.707314 -0.705222 -0.629762  0.204466  0.050051 -0.180452   \n1    -0.195432  0.135196 -0.871310  0.261822  0.204466 -0.216586 -0.260307   \n2    -0.445519  0.394123 -0.579806  0.315952  0.204466 -0.090330 -0.222494   \n3    -0.487200 -0.178382 -0.608399 -0.467579  0.204466 -0.066770 -0.215438   \n4     0.033814 -0.693454 -0.821721 -0.815304  0.204466 -0.313574 -0.289354   \n...        ...       ...       ...       ...       ...       ...       ...   \n1995  0.471466 -0.325083 -0.306984 -0.274259  0.204466 -0.462384 -0.333920   \n1996  0.283901  1.165656 -1.618302  1.200351  2.776360 -0.403578  4.627703   \n1997  0.033814 -0.434802 -0.753341 -0.527014  0.204466 -0.313574 -0.289354   \n1998  0.742393 -0.216432 -0.010487 -0.173414  0.204466 -0.536708 -0.356180   \n1999  0.659031  0.055048 -1.503930  0.006584  2.776360 -0.515036  4.933690   \n\n             7         8         9        10        11        12  label  \n0    -0.139112 -0.035459  0.159406  0.057477  1.056521  1.182540      0  \n1    -0.228194 -0.049212  0.508953 -0.313972  0.132240  0.623127      0  \n2    -0.186013 -0.064573  0.766199 -0.815762  0.569897  0.888015      0  \n3    -0.178141  0.103124 -0.111545  0.354241  0.651569  0.937446      0  \n4    -0.260597 -0.019844  0.264828  0.213477 -0.203965  0.419642      0  \n...        ...       ...       ...       ...       ...       ...    ...  \n1995 -0.198254  0.363286  0.040553 -0.079189  0.442881  0.107436      1  \n1996  4.690855 -1.686755 -3.311491 -2.058166 -4.207862 -2.003678      1  \n1997 -0.260597  0.032857  0.711325 -0.638389 -0.203965  0.419642      1  \n1998 -0.335144 -0.563019  0.419580  0.996506  0.099360 -0.048499      1  \n1999  4.716369 -1.738740 -3.316877 -2.058166 -4.207862 -2.003678      1  \n\n[2000 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.674765</td>\n      <td>-0.707314</td>\n      <td>-0.705222</td>\n      <td>-0.629762</td>\n      <td>0.204466</td>\n      <td>0.050051</td>\n      <td>-0.180452</td>\n      <td>-0.139112</td>\n      <td>-0.035459</td>\n      <td>0.159406</td>\n      <td>0.057477</td>\n      <td>1.056521</td>\n      <td>1.182540</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.195432</td>\n      <td>0.135196</td>\n      <td>-0.871310</td>\n      <td>0.261822</td>\n      <td>0.204466</td>\n      <td>-0.216586</td>\n      <td>-0.260307</td>\n      <td>-0.228194</td>\n      <td>-0.049212</td>\n      <td>0.508953</td>\n      <td>-0.313972</td>\n      <td>0.132240</td>\n      <td>0.623127</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.445519</td>\n      <td>0.394123</td>\n      <td>-0.579806</td>\n      <td>0.315952</td>\n      <td>0.204466</td>\n      <td>-0.090330</td>\n      <td>-0.222494</td>\n      <td>-0.186013</td>\n      <td>-0.064573</td>\n      <td>0.766199</td>\n      <td>-0.815762</td>\n      <td>0.569897</td>\n      <td>0.888015</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.487200</td>\n      <td>-0.178382</td>\n      <td>-0.608399</td>\n      <td>-0.467579</td>\n      <td>0.204466</td>\n      <td>-0.066770</td>\n      <td>-0.215438</td>\n      <td>-0.178141</td>\n      <td>0.103124</td>\n      <td>-0.111545</td>\n      <td>0.354241</td>\n      <td>0.651569</td>\n      <td>0.937446</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.033814</td>\n      <td>-0.693454</td>\n      <td>-0.821721</td>\n      <td>-0.815304</td>\n      <td>0.204466</td>\n      <td>-0.313574</td>\n      <td>-0.289354</td>\n      <td>-0.260597</td>\n      <td>-0.019844</td>\n      <td>0.264828</td>\n      <td>0.213477</td>\n      <td>-0.203965</td>\n      <td>0.419642</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>0.471466</td>\n      <td>-0.325083</td>\n      <td>-0.306984</td>\n      <td>-0.274259</td>\n      <td>0.204466</td>\n      <td>-0.462384</td>\n      <td>-0.333920</td>\n      <td>-0.198254</td>\n      <td>0.363286</td>\n      <td>0.040553</td>\n      <td>-0.079189</td>\n      <td>0.442881</td>\n      <td>0.107436</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>0.283901</td>\n      <td>1.165656</td>\n      <td>-1.618302</td>\n      <td>1.200351</td>\n      <td>2.776360</td>\n      <td>-0.403578</td>\n      <td>4.627703</td>\n      <td>4.690855</td>\n      <td>-1.686755</td>\n      <td>-3.311491</td>\n      <td>-2.058166</td>\n      <td>-4.207862</td>\n      <td>-2.003678</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>0.033814</td>\n      <td>-0.434802</td>\n      <td>-0.753341</td>\n      <td>-0.527014</td>\n      <td>0.204466</td>\n      <td>-0.313574</td>\n      <td>-0.289354</td>\n      <td>-0.260597</td>\n      <td>0.032857</td>\n      <td>0.711325</td>\n      <td>-0.638389</td>\n      <td>-0.203965</td>\n      <td>0.419642</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>0.742393</td>\n      <td>-0.216432</td>\n      <td>-0.010487</td>\n      <td>-0.173414</td>\n      <td>0.204466</td>\n      <td>-0.536708</td>\n      <td>-0.356180</td>\n      <td>-0.335144</td>\n      <td>-0.563019</td>\n      <td>0.419580</td>\n      <td>0.996506</td>\n      <td>0.099360</td>\n      <td>-0.048499</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>0.659031</td>\n      <td>0.055048</td>\n      <td>-1.503930</td>\n      <td>0.006584</td>\n      <td>2.776360</td>\n      <td>-0.515036</td>\n      <td>4.933690</td>\n      <td>4.716369</td>\n      <td>-1.738740</td>\n      <td>-3.316877</td>\n      <td>-2.058166</td>\n      <td>-4.207862</td>\n      <td>-2.003678</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows Ã— 14 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_dataset.drop(columns=['label']))\n",
    "scaled_dataset = scaler.transform(train_dataset.drop(columns=['label']))\n",
    "scaled_dataset = pd.DataFrame(scaled_dataset, columns=train_dataset.drop(columns=['label']).columns)\n",
    "train_dataset = pd.concat([scaled_dataset, train_dataset['label']], axis=1)\n",
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:11:19.626588600Z",
     "start_time": "2023-10-12T14:11:19.576586100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_dataset.drop(columns=['label']), train_dataset['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:11:19.707625500Z",
     "start_time": "2023-10-12T14:11:19.622586500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Average Loss: 0.6847\n",
      "Epoch [2/100], Average Loss: 0.6724\n",
      "Epoch [3/100], Average Loss: 0.6708\n",
      "Epoch [4/100], Average Loss: 0.6712\n",
      "Epoch [5/100], Average Loss: 0.6740\n",
      "Epoch [6/100], Average Loss: 0.6700\n",
      "Epoch [7/100], Average Loss: 0.6688\n",
      "Epoch [8/100], Average Loss: 0.6698\n",
      "Epoch [9/100], Average Loss: 0.6697\n",
      "Epoch [10/100], Average Loss: 0.6680\n",
      "Epoch [11/100], Average Loss: 0.6686\n",
      "Epoch [12/100], Average Loss: 0.6685\n",
      "Epoch [13/100], Average Loss: 0.6675\n",
      "Epoch [14/100], Average Loss: 0.6671\n",
      "Epoch [15/100], Average Loss: 0.6680\n",
      "Epoch [16/100], Average Loss: 0.6693\n",
      "Epoch [17/100], Average Loss: 0.6670\n",
      "Epoch [18/100], Average Loss: 0.6686\n",
      "Epoch [19/100], Average Loss: 0.6685\n",
      "Epoch [20/100], Average Loss: 0.6669\n",
      "Epoch [21/100], Average Loss: 0.6671\n",
      "Epoch [22/100], Average Loss: 0.6675\n",
      "Epoch [23/100], Average Loss: 0.6667\n",
      "Epoch [24/100], Average Loss: 0.6681\n",
      "Epoch [25/100], Average Loss: 0.6668\n",
      "Epoch [26/100], Average Loss: 0.6685\n",
      "Epoch [27/100], Average Loss: 0.6676\n",
      "Epoch [28/100], Average Loss: 0.6675\n",
      "Epoch [29/100], Average Loss: 0.6671\n",
      "Epoch [30/100], Average Loss: 0.6674\n",
      "Epoch [31/100], Average Loss: 0.6666\n",
      "Epoch [32/100], Average Loss: 0.6698\n",
      "Epoch [33/100], Average Loss: 0.6677\n",
      "Epoch [34/100], Average Loss: 0.6678\n",
      "Epoch [35/100], Average Loss: 0.6686\n",
      "Epoch [36/100], Average Loss: 0.6664\n",
      "Epoch [37/100], Average Loss: 0.6671\n",
      "Epoch [38/100], Average Loss: 0.6673\n",
      "Epoch [39/100], Average Loss: 0.6681\n",
      "Epoch [40/100], Average Loss: 0.6665\n",
      "Epoch [41/100], Average Loss: 0.6667\n",
      "Epoch [42/100], Average Loss: 0.6660\n",
      "Epoch [43/100], Average Loss: 0.6687\n",
      "Epoch [44/100], Average Loss: 0.6676\n",
      "Epoch [45/100], Average Loss: 0.6667\n",
      "Epoch [46/100], Average Loss: 0.6676\n",
      "Epoch [47/100], Average Loss: 0.6665\n",
      "Epoch [48/100], Average Loss: 0.6668\n",
      "Epoch [49/100], Average Loss: 0.6670\n",
      "Epoch [50/100], Average Loss: 0.6689\n",
      "Epoch [51/100], Average Loss: 0.6675\n",
      "Epoch [52/100], Average Loss: 0.6685\n",
      "Epoch [53/100], Average Loss: 0.6678\n",
      "Epoch [54/100], Average Loss: 0.6669\n",
      "Epoch [55/100], Average Loss: 0.6682\n",
      "Epoch [56/100], Average Loss: 0.6669\n",
      "Epoch [57/100], Average Loss: 0.6675\n",
      "Epoch [58/100], Average Loss: 0.6669\n",
      "Epoch [59/100], Average Loss: 0.6678\n",
      "Epoch [60/100], Average Loss: 0.6667\n",
      "Epoch [61/100], Average Loss: 0.6682\n",
      "Epoch [62/100], Average Loss: 0.6674\n",
      "Epoch [63/100], Average Loss: 0.6669\n",
      "Epoch [64/100], Average Loss: 0.6680\n",
      "Epoch [65/100], Average Loss: 0.6671\n",
      "Epoch [66/100], Average Loss: 0.6676\n",
      "Epoch [67/100], Average Loss: 0.6673\n",
      "Epoch [68/100], Average Loss: 0.6687\n",
      "Epoch [69/100], Average Loss: 0.6674\n",
      "Epoch [70/100], Average Loss: 0.6667\n",
      "Epoch [71/100], Average Loss: 0.6680\n",
      "Epoch [72/100], Average Loss: 0.6685\n",
      "Epoch [73/100], Average Loss: 0.6665\n",
      "Epoch [74/100], Average Loss: 0.6676\n",
      "Epoch [75/100], Average Loss: 0.6681\n",
      "Epoch [76/100], Average Loss: 0.6677\n",
      "Epoch [77/100], Average Loss: 0.6683\n",
      "Epoch [78/100], Average Loss: 0.6685\n",
      "Epoch [79/100], Average Loss: 0.6658\n",
      "Epoch [80/100], Average Loss: 0.6672\n",
      "Epoch [81/100], Average Loss: 0.6681\n",
      "Epoch [82/100], Average Loss: 0.6694\n",
      "Epoch [83/100], Average Loss: 0.6665\n",
      "Epoch [84/100], Average Loss: 0.6675\n",
      "Epoch [85/100], Average Loss: 0.6667\n",
      "Epoch [86/100], Average Loss: 0.6669\n",
      "Epoch [87/100], Average Loss: 0.6668\n",
      "Epoch [88/100], Average Loss: 0.6690\n",
      "Epoch [89/100], Average Loss: 0.6676\n",
      "Epoch [90/100], Average Loss: 0.6671\n",
      "Epoch [91/100], Average Loss: 0.6668\n",
      "Epoch [92/100], Average Loss: 0.6678\n",
      "Epoch [93/100], Average Loss: 0.6666\n",
      "Epoch [94/100], Average Loss: 0.6693\n",
      "Epoch [95/100], Average Loss: 0.6676\n",
      "Epoch [96/100], Average Loss: 0.6685\n",
      "Epoch [97/100], Average Loss: 0.6670\n",
      "Epoch [98/100], Average Loss: 0.6692\n",
      "Epoch [99/100], Average Loss: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 17:11:28,368\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Average Loss: 0.6677\n",
      "Epoch [1/100], Average Loss: 0.6677\n",
      "Epoch [2/100], Average Loss: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SAMPO\\venv\\lib\\site-packages\\ray\\air\\session.py:28: UserWarning: `report` is meant to only be called inside a function that is executed by a Tuner or Trainer. Returning `None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Average Loss: 0.6667\n",
      "Epoch [4/100], Average Loss: 0.6675\n",
      "Epoch [5/100], Average Loss: 0.6672\n",
      "Epoch [6/100], Average Loss: 0.6694\n",
      "Epoch [7/100], Average Loss: 0.6677\n",
      "Epoch [8/100], Average Loss: 0.6692\n",
      "Epoch [9/100], Average Loss: 0.6690\n",
      "Epoch [10/100], Average Loss: 0.6670\n",
      "Epoch [11/100], Average Loss: 0.6672\n",
      "Epoch [12/100], Average Loss: 0.6668\n",
      "Epoch [13/100], Average Loss: 0.6668\n",
      "Epoch [14/100], Average Loss: 0.6674\n",
      "Epoch [15/100], Average Loss: 0.6689\n",
      "Epoch [16/100], Average Loss: 0.6669\n",
      "Epoch [17/100], Average Loss: 0.6672\n",
      "Epoch [18/100], Average Loss: 0.6672\n",
      "Epoch [19/100], Average Loss: 0.6684\n",
      "Epoch [20/100], Average Loss: 0.6671\n",
      "Epoch [21/100], Average Loss: 0.6679\n",
      "Epoch [22/100], Average Loss: 0.6687\n",
      "Epoch [23/100], Average Loss: 0.6668\n",
      "Epoch [24/100], Average Loss: 0.6678\n",
      "Epoch [25/100], Average Loss: 0.6667\n",
      "Epoch [26/100], Average Loss: 0.6691\n",
      "Epoch [27/100], Average Loss: 0.6667\n",
      "Epoch [28/100], Average Loss: 0.6665\n",
      "Epoch [29/100], Average Loss: 0.6674\n",
      "Epoch [30/100], Average Loss: 0.6675\n",
      "Epoch [31/100], Average Loss: 0.6668\n",
      "Epoch [32/100], Average Loss: 0.6667\n",
      "Epoch [33/100], Average Loss: 0.6664\n",
      "Epoch [34/100], Average Loss: 0.6677\n",
      "Epoch [35/100], Average Loss: 0.6672\n",
      "Epoch [36/100], Average Loss: 0.6673\n",
      "Epoch [37/100], Average Loss: 0.6664\n",
      "Epoch [38/100], Average Loss: 0.6684\n",
      "Epoch [39/100], Average Loss: 0.6667\n",
      "Epoch [40/100], Average Loss: 0.6668\n",
      "Epoch [41/100], Average Loss: 0.6663\n",
      "Epoch [42/100], Average Loss: 0.6670\n",
      "Epoch [43/100], Average Loss: 0.6665\n",
      "Epoch [44/100], Average Loss: 0.6657\n",
      "Epoch [45/100], Average Loss: 0.6673\n",
      "Epoch [46/100], Average Loss: 0.6672\n",
      "Epoch [47/100], Average Loss: 0.6656\n",
      "Epoch [48/100], Average Loss: 0.6653\n",
      "Epoch [49/100], Average Loss: 0.6662\n",
      "Epoch [50/100], Average Loss: 0.6670\n",
      "Epoch [51/100], Average Loss: 0.6674\n",
      "Epoch [52/100], Average Loss: 0.6653\n",
      "Epoch [53/100], Average Loss: 0.6663\n",
      "Epoch [54/100], Average Loss: 0.6665\n",
      "Epoch [55/100], Average Loss: 0.6663\n",
      "Epoch [56/100], Average Loss: 0.6661\n",
      "Epoch [57/100], Average Loss: 0.6656\n",
      "Epoch [58/100], Average Loss: 0.6671\n",
      "Epoch [59/100], Average Loss: 0.6657\n",
      "Epoch [60/100], Average Loss: 0.6664\n",
      "Epoch [61/100], Average Loss: 0.6655\n",
      "Epoch [62/100], Average Loss: 0.6663\n",
      "Epoch [63/100], Average Loss: 0.6667\n",
      "Epoch [64/100], Average Loss: 0.6668\n",
      "Epoch [65/100], Average Loss: 0.6683\n",
      "Epoch [66/100], Average Loss: 0.6663\n",
      "Epoch [67/100], Average Loss: 0.6661\n",
      "Epoch [68/100], Average Loss: 0.6662\n",
      "Epoch [69/100], Average Loss: 0.6661\n",
      "Epoch [70/100], Average Loss: 0.6662\n",
      "Epoch [71/100], Average Loss: 0.6659\n",
      "Epoch [72/100], Average Loss: 0.6655\n",
      "Epoch [73/100], Average Loss: 0.6656\n",
      "Epoch [74/100], Average Loss: 0.6652\n",
      "Epoch [75/100], Average Loss: 0.6657\n",
      "Epoch [76/100], Average Loss: 0.6653\n",
      "Epoch [77/100], Average Loss: 0.6654\n",
      "Epoch [78/100], Average Loss: 0.6656\n",
      "Epoch [79/100], Average Loss: 0.6654\n",
      "Epoch [80/100], Average Loss: 0.6652\n",
      "Epoch [81/100], Average Loss: 0.6657\n",
      "Epoch [82/100], Average Loss: 0.6656\n",
      "Epoch [83/100], Average Loss: 0.6650\n",
      "Epoch [84/100], Average Loss: 0.6654\n",
      "Epoch [85/100], Average Loss: 0.6680\n",
      "Epoch [86/100], Average Loss: 0.6666\n",
      "Epoch [87/100], Average Loss: 0.6670\n",
      "Epoch [88/100], Average Loss: 0.6651\n",
      "Epoch [89/100], Average Loss: 0.6664\n",
      "Epoch [90/100], Average Loss: 0.6654\n",
      "Epoch [91/100], Average Loss: 0.6658\n",
      "Epoch [92/100], Average Loss: 0.6665\n",
      "Epoch [93/100], Average Loss: 0.6664\n",
      "Epoch [94/100], Average Loss: 0.6677\n",
      "Epoch [95/100], Average Loss: 0.6662\n",
      "Epoch [96/100], Average Loss: 0.6659\n",
      "Epoch [97/100], Average Loss: 0.6658\n",
      "Epoch [98/100], Average Loss: 0.6661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 17:11:37,120\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100], Average Loss: 0.6658\n",
      "Epoch [100/100], Average Loss: 0.6655\n",
      "Epoch [1/100], Average Loss: 0.6668\n",
      "Epoch [2/100], Average Loss: 0.6678\n",
      "Epoch [3/100], Average Loss: 0.6671\n",
      "Epoch [4/100], Average Loss: 0.6669\n",
      "Epoch [5/100], Average Loss: 0.6662\n",
      "Epoch [6/100], Average Loss: 0.6666\n",
      "Epoch [7/100], Average Loss: 0.6660\n",
      "Epoch [8/100], Average Loss: 0.6662\n",
      "Epoch [9/100], Average Loss: 0.6674\n",
      "Epoch [10/100], Average Loss: 0.6666\n",
      "Epoch [11/100], Average Loss: 0.6662\n",
      "Epoch [12/100], Average Loss: 0.6680\n",
      "Epoch [13/100], Average Loss: 0.6672\n",
      "Epoch [14/100], Average Loss: 0.6663\n",
      "Epoch [15/100], Average Loss: 0.6660\n",
      "Epoch [16/100], Average Loss: 0.6663\n",
      "Epoch [17/100], Average Loss: 0.6693\n",
      "Epoch [18/100], Average Loss: 0.6675\n",
      "Epoch [19/100], Average Loss: 0.6662\n",
      "Epoch [20/100], Average Loss: 0.6667\n",
      "Epoch [21/100], Average Loss: 0.6667\n",
      "Epoch [22/100], Average Loss: 0.6689\n",
      "Epoch [23/100], Average Loss: 0.6665\n",
      "Epoch [24/100], Average Loss: 0.6675\n",
      "Epoch [25/100], Average Loss: 0.6660\n",
      "Epoch [26/100], Average Loss: 0.6667\n",
      "Epoch [27/100], Average Loss: 0.6660\n",
      "Epoch [28/100], Average Loss: 0.6665\n",
      "Epoch [29/100], Average Loss: 0.6664\n",
      "Epoch [30/100], Average Loss: 0.6661\n",
      "Epoch [31/100], Average Loss: 0.6661\n",
      "Epoch [32/100], Average Loss: 0.6661\n",
      "Epoch [33/100], Average Loss: 0.6656\n",
      "Epoch [34/100], Average Loss: 0.6667\n",
      "Epoch [35/100], Average Loss: 0.6662\n",
      "Epoch [36/100], Average Loss: 0.6668\n",
      "Epoch [37/100], Average Loss: 0.6674\n",
      "Epoch [38/100], Average Loss: 0.6662\n",
      "Epoch [39/100], Average Loss: 0.6666\n",
      "Epoch [40/100], Average Loss: 0.6672\n",
      "Epoch [41/100], Average Loss: 0.6656\n",
      "Epoch [42/100], Average Loss: 0.6658\n",
      "Epoch [43/100], Average Loss: 0.6671\n",
      "Epoch [44/100], Average Loss: 0.6663\n",
      "Epoch [45/100], Average Loss: 0.6671\n",
      "Epoch [46/100], Average Loss: 0.6657\n",
      "Epoch [47/100], Average Loss: 0.6664\n",
      "Epoch [48/100], Average Loss: 0.6669\n",
      "Epoch [49/100], Average Loss: 0.6666\n",
      "Epoch [50/100], Average Loss: 0.6667\n",
      "Epoch [51/100], Average Loss: 0.6666\n",
      "Epoch [52/100], Average Loss: 0.6682\n",
      "Epoch [53/100], Average Loss: 0.6672\n",
      "Epoch [54/100], Average Loss: 0.6662\n",
      "Epoch [55/100], Average Loss: 0.6660\n",
      "Epoch [56/100], Average Loss: 0.6673\n",
      "Epoch [57/100], Average Loss: 0.6661\n",
      "Epoch [58/100], Average Loss: 0.6660\n",
      "Epoch [59/100], Average Loss: 0.6675\n",
      "Epoch [60/100], Average Loss: 0.6671\n",
      "Epoch [61/100], Average Loss: 0.6655\n",
      "Epoch [62/100], Average Loss: 0.6665\n",
      "Epoch [63/100], Average Loss: 0.6664\n",
      "Epoch [64/100], Average Loss: 0.6658\n",
      "Epoch [65/100], Average Loss: 0.6664\n",
      "Epoch [66/100], Average Loss: 0.6668\n",
      "Epoch [67/100], Average Loss: 0.6670\n",
      "Epoch [68/100], Average Loss: 0.6670\n",
      "Epoch [69/100], Average Loss: 0.6652\n",
      "Epoch [70/100], Average Loss: 0.6661\n",
      "Epoch [71/100], Average Loss: 0.6665\n",
      "Epoch [72/100], Average Loss: 0.6655\n",
      "Epoch [73/100], Average Loss: 0.6669\n",
      "Epoch [74/100], Average Loss: 0.6667\n",
      "Epoch [75/100], Average Loss: 0.6658\n",
      "Epoch [76/100], Average Loss: 0.6661\n",
      "Epoch [77/100], Average Loss: 0.6667\n",
      "Epoch [78/100], Average Loss: 0.6657\n",
      "Epoch [79/100], Average Loss: 0.6661\n",
      "Epoch [80/100], Average Loss: 0.6677\n",
      "Epoch [81/100], Average Loss: 0.6666\n",
      "Epoch [82/100], Average Loss: 0.6669\n",
      "Epoch [83/100], Average Loss: 0.6675\n",
      "Epoch [84/100], Average Loss: 0.6667\n",
      "Epoch [85/100], Average Loss: 0.6665\n",
      "Epoch [86/100], Average Loss: 0.6660\n",
      "Epoch [87/100], Average Loss: 0.6668\n",
      "Epoch [88/100], Average Loss: 0.6664\n",
      "Epoch [89/100], Average Loss: 0.6661\n",
      "Epoch [90/100], Average Loss: 0.6669\n",
      "Epoch [91/100], Average Loss: 0.6665\n",
      "Epoch [92/100], Average Loss: 0.6658\n",
      "Epoch [93/100], Average Loss: 0.6659\n",
      "Epoch [94/100], Average Loss: 0.6665\n",
      "Epoch [95/100], Average Loss: 0.6660\n",
      "Epoch [96/100], Average Loss: 0.6665\n",
      "Epoch [97/100], Average Loss: 0.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 17:11:45,792\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Average Loss: 0.6661\n",
      "Epoch [99/100], Average Loss: 0.6654\n",
      "Epoch [100/100], Average Loss: 0.6656\n",
      "Epoch [1/100], Average Loss: 0.6657\n",
      "Epoch [2/100], Average Loss: 0.6656\n",
      "Epoch [3/100], Average Loss: 0.6656\n",
      "Epoch [4/100], Average Loss: 0.6661\n",
      "Epoch [5/100], Average Loss: 0.6668\n",
      "Epoch [6/100], Average Loss: 0.6667\n",
      "Epoch [7/100], Average Loss: 0.6656\n",
      "Epoch [8/100], Average Loss: 0.6664\n",
      "Epoch [9/100], Average Loss: 0.6659\n",
      "Epoch [10/100], Average Loss: 0.6662\n",
      "Epoch [11/100], Average Loss: 0.6665\n",
      "Epoch [12/100], Average Loss: 0.6664\n",
      "Epoch [13/100], Average Loss: 0.6659\n",
      "Epoch [14/100], Average Loss: 0.6655\n",
      "Epoch [15/100], Average Loss: 0.6681\n",
      "Epoch [16/100], Average Loss: 0.6661\n",
      "Epoch [17/100], Average Loss: 0.6665\n",
      "Epoch [18/100], Average Loss: 0.6657\n",
      "Epoch [19/100], Average Loss: 0.6669\n",
      "Epoch [20/100], Average Loss: 0.6659\n",
      "Epoch [21/100], Average Loss: 0.6659\n",
      "Epoch [22/100], Average Loss: 0.6666\n",
      "Epoch [23/100], Average Loss: 0.6675\n",
      "Epoch [24/100], Average Loss: 0.6658\n",
      "Epoch [25/100], Average Loss: 0.6667\n",
      "Epoch [26/100], Average Loss: 0.6663\n",
      "Epoch [27/100], Average Loss: 0.6658\n",
      "Epoch [28/100], Average Loss: 0.6659\n",
      "Epoch [29/100], Average Loss: 0.6652\n",
      "Epoch [30/100], Average Loss: 0.6657\n",
      "Epoch [31/100], Average Loss: 0.6652\n",
      "Epoch [32/100], Average Loss: 0.6668\n",
      "Epoch [33/100], Average Loss: 0.6668\n",
      "Epoch [34/100], Average Loss: 0.6660\n",
      "Epoch [35/100], Average Loss: 0.6670\n",
      "Epoch [36/100], Average Loss: 0.6667\n",
      "Epoch [37/100], Average Loss: 0.6667\n",
      "Epoch [38/100], Average Loss: 0.6652\n",
      "Epoch [39/100], Average Loss: 0.6673\n",
      "Epoch [40/100], Average Loss: 0.6658\n",
      "Epoch [41/100], Average Loss: 0.6653\n",
      "Epoch [42/100], Average Loss: 0.6649\n",
      "Epoch [43/100], Average Loss: 0.6651\n",
      "Epoch [44/100], Average Loss: 0.6652\n",
      "Epoch [45/100], Average Loss: 0.6655\n",
      "Epoch [46/100], Average Loss: 0.6654\n",
      "Epoch [47/100], Average Loss: 0.6655\n",
      "Epoch [48/100], Average Loss: 0.6663\n",
      "Epoch [49/100], Average Loss: 0.6661\n",
      "Epoch [50/100], Average Loss: 0.6657\n",
      "Epoch [51/100], Average Loss: 0.6651\n",
      "Epoch [52/100], Average Loss: 0.6658\n",
      "Epoch [53/100], Average Loss: 0.6673\n",
      "Epoch [54/100], Average Loss: 0.6654\n",
      "Epoch [55/100], Average Loss: 0.6651\n",
      "Epoch [56/100], Average Loss: 0.6655\n",
      "Epoch [57/100], Average Loss: 0.6653\n",
      "Epoch [58/100], Average Loss: 0.6653\n",
      "Epoch [59/100], Average Loss: 0.6653\n",
      "Epoch [60/100], Average Loss: 0.6651\n",
      "Epoch [61/100], Average Loss: 0.6650\n",
      "Epoch [62/100], Average Loss: 0.6650\n",
      "Epoch [63/100], Average Loss: 0.6651\n",
      "Epoch [64/100], Average Loss: 0.6653\n",
      "Epoch [65/100], Average Loss: 0.6658\n",
      "Epoch [66/100], Average Loss: 0.6651\n",
      "Epoch [67/100], Average Loss: 0.6656\n",
      "Epoch [68/100], Average Loss: 0.6662\n",
      "Epoch [69/100], Average Loss: 0.6655\n",
      "Epoch [70/100], Average Loss: 0.6650\n",
      "Epoch [71/100], Average Loss: 0.6651\n",
      "Epoch [72/100], Average Loss: 0.6654\n",
      "Epoch [73/100], Average Loss: 0.6650\n",
      "Epoch [74/100], Average Loss: 0.6650\n",
      "Epoch [75/100], Average Loss: 0.6650\n",
      "Epoch [76/100], Average Loss: 0.6650\n",
      "Epoch [77/100], Average Loss: 0.6663\n",
      "Epoch [78/100], Average Loss: 0.6659\n",
      "Epoch [79/100], Average Loss: 0.6653\n",
      "Epoch [80/100], Average Loss: 0.6656\n",
      "Epoch [81/100], Average Loss: 0.6653\n",
      "Epoch [82/100], Average Loss: 0.6653\n",
      "Epoch [83/100], Average Loss: 0.6653\n",
      "Epoch [84/100], Average Loss: 0.6650\n",
      "Epoch [85/100], Average Loss: 0.6658\n",
      "Epoch [86/100], Average Loss: 0.6652\n",
      "Epoch [87/100], Average Loss: 0.6651\n",
      "Epoch [88/100], Average Loss: 0.6647\n",
      "Epoch [89/100], Average Loss: 0.6659\n",
      "Epoch [90/100], Average Loss: 0.6660\n",
      "Epoch [91/100], Average Loss: 0.6656\n",
      "Epoch [92/100], Average Loss: 0.6663\n",
      "Epoch [93/100], Average Loss: 0.6655\n",
      "Epoch [94/100], Average Loss: 0.6672\n",
      "Epoch [95/100], Average Loss: 0.6658\n",
      "Epoch [96/100], Average Loss: 0.6650\n",
      "Epoch [97/100], Average Loss: 0.6649\n",
      "Epoch [98/100], Average Loss: 0.6652\n",
      "Epoch [99/100], Average Loss: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 17:11:54,513\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Average Loss: 0.6656\n",
      "Epoch [1/100], Average Loss: 0.6669\n",
      "Epoch [2/100], Average Loss: 0.6657\n",
      "Epoch [3/100], Average Loss: 0.6657\n",
      "Epoch [4/100], Average Loss: 0.6659\n",
      "Epoch [5/100], Average Loss: 0.6663\n",
      "Epoch [6/100], Average Loss: 0.6657\n",
      "Epoch [7/100], Average Loss: 0.6650\n",
      "Epoch [8/100], Average Loss: 0.6651\n",
      "Epoch [9/100], Average Loss: 0.6659\n",
      "Epoch [10/100], Average Loss: 0.6650\n",
      "Epoch [11/100], Average Loss: 0.6654\n",
      "Epoch [12/100], Average Loss: 0.6656\n",
      "Epoch [13/100], Average Loss: 0.6648\n",
      "Epoch [14/100], Average Loss: 0.6661\n",
      "Epoch [15/100], Average Loss: 0.6655\n",
      "Epoch [16/100], Average Loss: 0.6647\n",
      "Epoch [17/100], Average Loss: 0.6661\n",
      "Epoch [18/100], Average Loss: 0.6653\n",
      "Epoch [19/100], Average Loss: 0.6663\n",
      "Epoch [20/100], Average Loss: 0.6650\n",
      "Epoch [21/100], Average Loss: 0.6657\n",
      "Epoch [22/100], Average Loss: 0.6653\n",
      "Epoch [23/100], Average Loss: 0.6657\n",
      "Epoch [24/100], Average Loss: 0.6650\n",
      "Epoch [25/100], Average Loss: 0.6658\n",
      "Epoch [26/100], Average Loss: 0.6654\n",
      "Epoch [27/100], Average Loss: 0.6661\n",
      "Epoch [28/100], Average Loss: 0.6671\n",
      "Epoch [29/100], Average Loss: 0.6656\n",
      "Epoch [30/100], Average Loss: 0.6650\n",
      "Epoch [31/100], Average Loss: 0.6652\n",
      "Epoch [32/100], Average Loss: 0.6652\n",
      "Epoch [33/100], Average Loss: 0.6658\n",
      "Epoch [34/100], Average Loss: 0.6651\n",
      "Epoch [35/100], Average Loss: 0.6654\n",
      "Epoch [36/100], Average Loss: 0.6648\n",
      "Epoch [37/100], Average Loss: 0.6670\n",
      "Epoch [38/100], Average Loss: 0.6665\n",
      "Epoch [39/100], Average Loss: 0.6652\n",
      "Epoch [40/100], Average Loss: 0.6666\n",
      "Epoch [41/100], Average Loss: 0.6657\n",
      "Epoch [42/100], Average Loss: 0.6652\n",
      "Epoch [43/100], Average Loss: 0.6660\n",
      "Epoch [44/100], Average Loss: 0.6657\n",
      "Epoch [45/100], Average Loss: 0.6663\n",
      "Epoch [46/100], Average Loss: 0.6650\n",
      "Epoch [47/100], Average Loss: 0.6654\n",
      "Epoch [48/100], Average Loss: 0.6656\n",
      "Epoch [49/100], Average Loss: 0.6652\n",
      "Epoch [50/100], Average Loss: 0.6670\n",
      "Epoch [51/100], Average Loss: 0.6652\n",
      "Epoch [52/100], Average Loss: 0.6667\n",
      "Epoch [53/100], Average Loss: 0.6656\n",
      "Epoch [54/100], Average Loss: 0.6654\n",
      "Epoch [55/100], Average Loss: 0.6651\n",
      "Epoch [56/100], Average Loss: 0.6654\n",
      "Epoch [57/100], Average Loss: 0.6658\n",
      "Epoch [58/100], Average Loss: 0.6649\n",
      "Epoch [59/100], Average Loss: 0.6656\n",
      "Epoch [60/100], Average Loss: 0.6660\n",
      "Epoch [61/100], Average Loss: 0.6659\n",
      "Epoch [62/100], Average Loss: 0.6651\n",
      "Epoch [63/100], Average Loss: 0.6652\n",
      "Epoch [64/100], Average Loss: 0.6649\n",
      "Epoch [65/100], Average Loss: 0.6648\n",
      "Epoch [66/100], Average Loss: 0.6654\n",
      "Epoch [67/100], Average Loss: 0.6650\n",
      "Epoch [68/100], Average Loss: 0.6649\n",
      "Epoch [69/100], Average Loss: 0.6663\n",
      "Epoch [70/100], Average Loss: 0.6655\n",
      "Epoch [71/100], Average Loss: 0.6653\n",
      "Epoch [72/100], Average Loss: 0.6658\n",
      "Epoch [73/100], Average Loss: 0.6647\n",
      "Epoch [74/100], Average Loss: 0.6651\n",
      "Epoch [75/100], Average Loss: 0.6651\n",
      "Epoch [76/100], Average Loss: 0.6652\n",
      "Epoch [77/100], Average Loss: 0.6648\n",
      "Epoch [78/100], Average Loss: 0.6655\n",
      "Epoch [79/100], Average Loss: 0.6652\n",
      "Epoch [80/100], Average Loss: 0.6652\n",
      "Epoch [81/100], Average Loss: 0.6654\n",
      "Epoch [82/100], Average Loss: 0.6658\n",
      "Epoch [83/100], Average Loss: 0.6658\n",
      "Epoch [84/100], Average Loss: 0.6653\n",
      "Epoch [85/100], Average Loss: 0.6650\n",
      "Epoch [86/100], Average Loss: 0.6653\n",
      "Epoch [87/100], Average Loss: 0.6660\n",
      "Epoch [88/100], Average Loss: 0.6650\n",
      "Epoch [89/100], Average Loss: 0.6662\n",
      "Epoch [90/100], Average Loss: 0.6654\n",
      "Epoch [91/100], Average Loss: 0.6650\n",
      "Epoch [92/100], Average Loss: 0.6648\n",
      "Epoch [93/100], Average Loss: 0.6656\n",
      "Epoch [94/100], Average Loss: 0.6652\n",
      "Epoch [95/100], Average Loss: 0.6668\n",
      "Epoch [96/100], Average Loss: 0.6655\n",
      "Epoch [97/100], Average Loss: 0.6647\n",
      "Epoch [98/100], Average Loss: 0.6657\n",
      "Epoch [99/100], Average Loss: 0.6653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 17:12:03,222\tWARNING session.py:100 -- In neither tune session nor train session!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Average Loss: 0.6655\n"
     ]
    }
   ],
   "source": [
    "from sampo.scheduler.selection.neural_net import NeuralNetType\n",
    "from sampo.scheduler.selection.validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "s = cross_val_score(X=x_train,\n",
    "                    y=y_train,\n",
    "                    model=trainer,\n",
    "                    epochs=100,\n",
    "                    folds=5,\n",
    "                    shuffle=True,\n",
    "                    type_task=NeuralNetType.CLASSIFICATION)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:12:03.248586800Z",
     "start_time": "2023-10-12T14:11:19.664586Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:12:03.285587700Z",
     "start_time": "2023-10-12T14:12:03.237590400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(2.3059, grad_fn=<SelectBackward0>)\n",
      "1 tensor(1.9788, grad_fn=<SelectBackward0>)\n",
      "2 tensor(2.8585, grad_fn=<SelectBackward0>)\n",
      "3 tensor(1.6799, grad_fn=<SelectBackward0>)\n",
      "4 tensor(1.1946, grad_fn=<SelectBackward0>)\n",
      "5 tensor(1.8484, grad_fn=<SelectBackward0>)\n",
      "6 tensor(0.6657, grad_fn=<SelectBackward0>)\n",
      "7 tensor(0.7560, grad_fn=<SelectBackward0>)\n",
      "8 tensor(1.5524, grad_fn=<SelectBackward0>)\n",
      "9 tensor(1.2578, grad_fn=<SelectBackward0>)\n",
      "10 tensor(1.6093, grad_fn=<SelectBackward0>)\n",
      "11 tensor(1.8630, grad_fn=<SelectBackward0>)\n",
      "12 tensor(0.4621, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "index = [-10000] * input_parameters\n",
    "for fold in list(model.parameters())[0]:\n",
    "    for i in range(len(fold)):\n",
    "        index[i] = max(index[i], fold[i])\n",
    "for i in range(len(index)):\n",
    "    print(i, index[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:12:03.294586800Z",
     "start_time": "2023-10-12T14:12:03.253591100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5208333313465119, 0.6621743083000183, <sampo.scheduler.selection.neural_net.NeuralNetTrainer object at 0x000001E393FC6AD0>)\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:12:03.294586800Z",
     "start_time": "2023-10-12T14:12:03.267587600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "test_dataset = x_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:12:03.296589900Z",
     "start_time": "2023-10-12T14:12:03.283590Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6  \\\n1963  0.617350 -0.290791  0.350391 -0.294693  0.204466 -0.503817 -0.346329   \n983  -0.320476 -1.354054 -0.810233 -1.424877  0.204466 -0.156488 -0.242308   \n69   -2.133605  3.784072  2.115015  3.891530 -2.367429  2.847600  0.657385   \n1211  0.408944 -0.199342  0.364670 -0.361685  0.204466 -0.443517 -0.328270   \n1370  0.888277 -0.835995 -0.047870 -0.792034  0.204466 -0.572355 -0.366856   \n...        ...       ...       ...       ...       ...       ...       ...   \n1684  1.492653 -1.037543 -0.372736 -1.035875  0.204466 -0.694957 -0.403574   \n1771  1.263407 -1.242496  0.199215 -1.012485  0.204466 -0.652617 -0.390893   \n418   1.180045 -0.840916 -0.311822 -0.633079  0.204466 -0.636049 -0.385931   \n1560  1.034161 -0.480303  0.632971 -0.562725  0.204466 -0.605375 -0.376745   \n385   0.075495 -0.503840 -0.525408 -0.658796  0.204466 -0.329617 -0.294158   \n\n             7         8         9        10        11        12  \n1963 -0.324156 -0.366428  1.065895 -0.476924  1.366191  0.020507  \n983  -0.208116  0.003344  0.124182  0.199847  0.340565  0.749214  \n69    0.314857  2.138104 -1.485907 -1.704467  0.779448 -2.003678  \n1211 -0.304010 -0.660814  1.095033 -0.126086  0.530083  0.147019  \n1370 -0.347054 -0.755636  0.837893  0.732769 -0.065397 -0.123287  \n...        ...       ...       ...       ...       ...       ...  \n1684 -0.388014 -0.802895  0.450167  1.428771 -0.632055 -0.380511  \n1771 -0.373869 -1.013813  0.422268  1.887072 -0.436364 -0.291680  \n418  -0.368333 -0.681945  0.539356  1.148464 -0.359785 -0.256919  \n1560 -0.358086 -0.697739  0.975110  0.276246 -0.218014 -0.192565  \n385  -0.265957 -0.208688  0.434590  0.181926 -0.259575  0.385985  \n\n[500 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1963</th>\n      <td>0.617350</td>\n      <td>-0.290791</td>\n      <td>0.350391</td>\n      <td>-0.294693</td>\n      <td>0.204466</td>\n      <td>-0.503817</td>\n      <td>-0.346329</td>\n      <td>-0.324156</td>\n      <td>-0.366428</td>\n      <td>1.065895</td>\n      <td>-0.476924</td>\n      <td>1.366191</td>\n      <td>0.020507</td>\n    </tr>\n    <tr>\n      <th>983</th>\n      <td>-0.320476</td>\n      <td>-1.354054</td>\n      <td>-0.810233</td>\n      <td>-1.424877</td>\n      <td>0.204466</td>\n      <td>-0.156488</td>\n      <td>-0.242308</td>\n      <td>-0.208116</td>\n      <td>0.003344</td>\n      <td>0.124182</td>\n      <td>0.199847</td>\n      <td>0.340565</td>\n      <td>0.749214</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>-2.133605</td>\n      <td>3.784072</td>\n      <td>2.115015</td>\n      <td>3.891530</td>\n      <td>-2.367429</td>\n      <td>2.847600</td>\n      <td>0.657385</td>\n      <td>0.314857</td>\n      <td>2.138104</td>\n      <td>-1.485907</td>\n      <td>-1.704467</td>\n      <td>0.779448</td>\n      <td>-2.003678</td>\n    </tr>\n    <tr>\n      <th>1211</th>\n      <td>0.408944</td>\n      <td>-0.199342</td>\n      <td>0.364670</td>\n      <td>-0.361685</td>\n      <td>0.204466</td>\n      <td>-0.443517</td>\n      <td>-0.328270</td>\n      <td>-0.304010</td>\n      <td>-0.660814</td>\n      <td>1.095033</td>\n      <td>-0.126086</td>\n      <td>0.530083</td>\n      <td>0.147019</td>\n    </tr>\n    <tr>\n      <th>1370</th>\n      <td>0.888277</td>\n      <td>-0.835995</td>\n      <td>-0.047870</td>\n      <td>-0.792034</td>\n      <td>0.204466</td>\n      <td>-0.572355</td>\n      <td>-0.366856</td>\n      <td>-0.347054</td>\n      <td>-0.755636</td>\n      <td>0.837893</td>\n      <td>0.732769</td>\n      <td>-0.065397</td>\n      <td>-0.123287</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1684</th>\n      <td>1.492653</td>\n      <td>-1.037543</td>\n      <td>-0.372736</td>\n      <td>-1.035875</td>\n      <td>0.204466</td>\n      <td>-0.694957</td>\n      <td>-0.403574</td>\n      <td>-0.388014</td>\n      <td>-0.802895</td>\n      <td>0.450167</td>\n      <td>1.428771</td>\n      <td>-0.632055</td>\n      <td>-0.380511</td>\n    </tr>\n    <tr>\n      <th>1771</th>\n      <td>1.263407</td>\n      <td>-1.242496</td>\n      <td>0.199215</td>\n      <td>-1.012485</td>\n      <td>0.204466</td>\n      <td>-0.652617</td>\n      <td>-0.390893</td>\n      <td>-0.373869</td>\n      <td>-1.013813</td>\n      <td>0.422268</td>\n      <td>1.887072</td>\n      <td>-0.436364</td>\n      <td>-0.291680</td>\n    </tr>\n    <tr>\n      <th>418</th>\n      <td>1.180045</td>\n      <td>-0.840916</td>\n      <td>-0.311822</td>\n      <td>-0.633079</td>\n      <td>0.204466</td>\n      <td>-0.636049</td>\n      <td>-0.385931</td>\n      <td>-0.368333</td>\n      <td>-0.681945</td>\n      <td>0.539356</td>\n      <td>1.148464</td>\n      <td>-0.359785</td>\n      <td>-0.256919</td>\n    </tr>\n    <tr>\n      <th>1560</th>\n      <td>1.034161</td>\n      <td>-0.480303</td>\n      <td>0.632971</td>\n      <td>-0.562725</td>\n      <td>0.204466</td>\n      <td>-0.605375</td>\n      <td>-0.376745</td>\n      <td>-0.358086</td>\n      <td>-0.697739</td>\n      <td>0.975110</td>\n      <td>0.276246</td>\n      <td>-0.218014</td>\n      <td>-0.192565</td>\n    </tr>\n    <tr>\n      <th>385</th>\n      <td>0.075495</td>\n      <td>-0.503840</td>\n      <td>-0.525408</td>\n      <td>-0.658796</td>\n      <td>0.204466</td>\n      <td>-0.329617</td>\n      <td>-0.294158</td>\n      <td>-0.265957</td>\n      <td>-0.208688</td>\n      <td>0.434590</td>\n      <td>0.181926</td>\n      <td>-0.259575</td>\n      <td>0.385985</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows Ã— 13 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:12:03.360590100Z",
     "start_time": "2023-10-12T14:12:03.310590300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 1.], dtype=float32)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = trainer.predict([torch.Tensor(v) for v in test_dataset.iloc[0:2, :].values])\n",
    "predicted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:17:46.900925100Z",
     "start_time": "2023-10-12T14:17:46.886942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.float32"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predicted[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T14:19:15.151659800Z",
     "start_time": "2023-10-12T14:19:15.133659500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m label_test \u001B[38;5;241m=\u001B[39m y_test\u001B[38;5;241m.\u001B[39mto_numpy()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(predicted)):\n\u001B[1;32m----> 5\u001B[0m     array\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpredicted\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlabel_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28msum\u001B[39m(array) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(array)\n",
      "\u001B[1;31mTypeError\u001B[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "array = []\n",
    "label_test = y_test.to_numpy()\n",
    "for i in range(len(predicted)):\n",
    "    array.append(int(predicted[i] == label_test[i]))\n",
    "sum(array) / len(array)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T13:55:56.881262100Z",
     "start_time": "2023-10-12T13:55:56.748227500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1.], dtype=float32)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T10:07:20.040951300Z",
     "start_time": "2023-10-11T10:07:20.022972200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
